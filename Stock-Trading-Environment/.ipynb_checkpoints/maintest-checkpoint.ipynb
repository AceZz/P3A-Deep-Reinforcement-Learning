{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from gym import wrappers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQNac import MyModel\n",
    "from networks.DQNac import StockActor, StockCritic, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[2.21399994e-02 2.12740006e-02 2.14240006e-02 2.23740006e-02\n",
      "    2.24520004e-02 2.24320008e-02]\n",
      "   [2.23199996e-02 2.20320008e-02 2.19679992e-02 2.25300004e-02\n",
      "    2.26479996e-02 2.26979996e-02]\n",
      "   [2.12699996e-02 2.12520004e-02 2.13640000e-02 2.21320008e-02\n",
      "    2.23320008e-02 2.23940002e-02]\n",
      "   [2.16460006e-02 2.13500000e-02 2.18820008e-02 2.25300004e-02\n",
      "    2.23559998e-02 2.25880004e-02]\n",
      "   [3.13008670e-02 2.83078756e-02 2.48718076e-02 2.74769031e-02\n",
      "    4.11783345e-02 2.10327562e-02]\n",
      "   [4.65661288e-06 4.65661288e-06 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/AAPL.csv')\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# The algorithms require a vectorized environment to run\n",
    "env = StockTradingEnv(df)\n",
    "\n",
    "base_action = np.array([0,0.5])\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(5):\n",
    "    action, _states = (base_action, None)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "#print(obs)\n",
    "obs = env.reset()\n",
    "obs=np.expand_dims(obs,axis=0)\n",
    "obs=np.expand_dims(obs,axis=0)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mod = MyModel(32, (1,4), (1,1))\n",
    "\n",
    "print(obs)\n",
    "z = mod.predict(obs)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "act = StockActor()\n",
    "act_target = StockActor()\n",
    "output_action = act(obs)\n",
    "output_action_tar = act_target(obs)\n",
    "print(\"output_action\")\n",
    "print(output_action)\n",
    "print(\" \")\n",
    "print(\"output_action_tar\")\n",
    "print(output_action_tar)\n",
    "print(\" \")\n",
    "output_action = act(obs)\n",
    "output_action_tar = act_target(obs)\n",
    "print(\"output_action2\")\n",
    "print(tf.squeeze(output_action))\n",
    "print(\" \")\n",
    "print(\"output_action_tar2\")\n",
    "print(output_action_tar)\n",
    "print(\" \")\n",
    "\n",
    "#act.model.summary()\n",
    "#print(act.model.layers[3].get_weights())\n",
    "#act.model.layers[3].set_weights([np.array([[-0.3]], dtype=np.float32), np.array([0.1], dtype=np.float32)])\n",
    "#print(act.model.layers[3].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crit = StockCritic()\n",
    "act = StockActor()\n",
    "\n",
    "output_action = act(obs)\n",
    "print(output_action)\n",
    "output_action = tf.squeeze(output_action)\n",
    "output_crit = crit([obs,output_action])\n",
    "print(output_crit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "gamma = 0.99\n",
    "max_experiences = 1000\n",
    "min_experiences = 365\n",
    "batch_size = 32\n",
    "lr = 1e-2\n",
    "D = DQN( gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "observations = env.reset()\n",
    "observations = np.expand_dims(observations,axis=0)\n",
    "observations = np.expand_dims(observations,axis=0)\n",
    "D.act(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = act.model.trainable_variables\n",
    "print(a)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" hjkbedfhugqdfhiqdfbhiqdf\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "b = act.trainable_variables\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#model = MyModel(32, (1,4), (1,1))\n",
    "#output=tf.squeeze(model(obs))\n",
    "\n",
    "crit = StockCritic()\n",
    "crit_tar = StockCritic()\n",
    "crit.action = tf.squeeze(output_action)\n",
    "crit_tar.action = tf.squeeze(output_action)\n",
    "output = crit(obs)\n",
    "output_tar = crit_tar(obs)\n",
    "print(\"output\")\n",
    "print(output)\n",
    "print(\" \")\n",
    "print(\"output_tar\")\n",
    "print(output_tar)\n",
    "print(\" \")\n",
    "crit.action = tf.squeeze(output_action_tar)\n",
    "crit_tar.action = tf.squeeze(output_action)\n",
    "output = crit(obs)\n",
    "output_tar = crit_tar(obs)\n",
    "print(\"output2\")\n",
    "print(output)\n",
    "print(\" \")\n",
    "print(\"output_tar2\")\n",
    "print(output_tar)\n",
    "print(\" \")\n",
    "print(crit.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fill_buffer(env, DQN):\n",
    "    \n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    observations = np.expand_dims(observations,axis=0)\n",
    "    observations = np.expand_dims(observations,axis=0)\n",
    "    steps = 0\n",
    "    it=0\n",
    "    while it<DQN.min_experiences:\n",
    "        \n",
    "        #print(it)\n",
    "        #observations = env.reset()\n",
    "\n",
    "        action = DQN.act(observations) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = np.tanh(tf.squeeze(action)+np.random.normal(0,0.05)) #avoiding small action values in the begining ->[2 0] to frequently\n",
    "        #print(action)\n",
    "        action = DQN.convert_action(action)\n",
    "         \n",
    "        #print(action)\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        #print(reward)\n",
    "        observations = np.expand_dims(observations,axis=0)\n",
    "        observations = np.expand_dims(observations,axis=0)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "\n",
    "        if done :\n",
    "            print(\"DONE\")\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        steps += 1\n",
    "        if steps >= 300: # Limiting the number of steps\n",
    "            print(\"STEPS\")\n",
    "            observations = env.reset()\n",
    "            observations = np.expand_dims(observations,axis=0)\n",
    "            observations = np.expand_dims(observations,axis=0)\n",
    "            steps = 0\n",
    "        \n",
    "        obs = tf.squeeze(observations)\n",
    "        obs = np.expand_dims(obs,axis=0)\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': obs, 'done': done}\n",
    "        DQN.add_experience(exp)\n",
    "        \n",
    "        it += 1\n",
    "    return \" Done \"\n",
    "\n",
    "def DDPG():\n",
    "    \n",
    "    gamma = 0.99\n",
    "    max_experiences = 1000\n",
    "    min_experiences = 365\n",
    "    batch_size = 32\n",
    "    lr = 1e-2\n",
    "    \n",
    "    M = 1\n",
    "    T = 1\n",
    " \n",
    "    D = DQN( gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    a = fill_buffer(env, D)\n",
    "    #print(a)\n",
    "    \n",
    "    for i in range(M):\n",
    "        D.train(env,T)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer stock_actor_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[[[5.34857140e-03 5.20028580e-03 5.25342860e-03 5.33514280e-03\n",
      "   5.31800000e-03 5.29571420e-03]\n",
      "  [5.36285700e-03 5.30000000e-03 5.33371420e-03 5.35428580e-03\n",
      "   5.32714280e-03 5.32057160e-03]\n",
      "  [5.22200020e-03 5.18400000e-03 5.23800020e-03 5.26600000e-03\n",
      "   5.21742860e-03 5.16285700e-03]\n",
      "  [5.25200000e-03 5.21057120e-03 5.31857140e-03 5.29657140e-03\n",
      "   5.29571420e-03 5.16742860e-03]\n",
      "  [6.41309191e-02 5.18326648e-02 3.92837450e-02 4.02081758e-02\n",
      "   6.28159847e-02 6.10844698e-02]\n",
      "  [4.38526032e-06 4.65661288e-06 1.02445483e-08 5.29750155e-03\n",
      "   0.00000000e+00 0.00000000e+00]]]\n",
      "STEPS\n",
      "prev_observations\n",
      "[[[[2.99119996e-02 3.02579986e-02 3.05000000e-02 3.05760010e-02\n",
      "    3.01700012e-02 3.00540008e-02]\n",
      "   [3.03640014e-02 3.09059998e-02 3.07940002e-02 3.07399994e-02\n",
      "    3.02540008e-02 3.06779998e-02]\n",
      "   [2.97040008e-02 2.99260010e-02 3.01720002e-02 3.03019990e-02\n",
      "    2.98440002e-02 3.00100006e-02]\n",
      "   [3.01500000e-02 3.06619996e-02 3.07600006e-02 3.04579986e-02\n",
      "    3.00000000e-02 3.06140014e-02]\n",
      "   [1.91038940e-02 2.10009050e-02 1.66616868e-02 1.25836581e-02\n",
      "    1.51056796e-02 1.33695547e-02]\n",
      "   [4.65661288e-06 4.65661288e-06 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (6,6) into shape (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-58d0c5c70b23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDDPG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-2a89a11e045a>\u001b[0m in \u001b[0;36mDDPG\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\networks\\DQNac.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, env, T)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_action_back\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (6,6) into shape (1)"
     ]
    }
   ],
   "source": [
    "DDPG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (reinforcement-learning-master)",
   "language": "python",
   "name": "pycharm-7a748578"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
