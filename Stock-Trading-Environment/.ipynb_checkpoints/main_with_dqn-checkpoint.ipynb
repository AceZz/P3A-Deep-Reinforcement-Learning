{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN\n",
    "from utils.utils import convert_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "N_games = 50000  # number of training games\n",
    "N_save = 200    # interval between save model, must be over 200\n",
    "model_name = \"DQN\"\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "hidden_units = [64,128,256,256,128,64]\n",
    "in_log = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = convert_action(action)                # TODO: REMOVE THIS\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "#         rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# def make_video(env, TrainNet):\n",
    "#     env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "#     rewards = 0\n",
    "#     steps = 0\n",
    "#     done = False\n",
    "#     observation = env.reset()\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = TrainNet.get_action(observation, 0)\n",
    "#         action = convert_action(action)                # TODO: REMOVE THIS\n",
    "#         observation, reward, done, _ = env.step(action)\n",
    "#         steps += 1\n",
    "#         rewards += reward\n",
    "#     print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('./data/AAPL_train.csv')\n",
    "    df = df.sort_values('Date')\n",
    "    env = StockTradingEnv(df, in_log=in_log)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 3                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N_games = 50000\n",
    "    total_rewards = np.empty(N_games)\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N_games):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "        \n",
    "        # Save the model\n",
    "        if n % N_save == 0 and n>=N_save:\n",
    "            TrainNet.model.save_weights('save_models/{}_{}'.format(model_name, n), save_format='tf')\n",
    "\n",
    "        \n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the difference of net_worth between the beginning and the end of the step\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tristan\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py:90: RuntimeWarning: divide by zero encountered in log\n",
      "  obs = np.log(obs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: -835.3362477278388 eps: 0.8991 avg reward (last 100): -835.3362477278388\n",
      "episode: 100 episode reward: 1050.142873603534 eps: 0.8134986194699355 avg reward (last 100): 513.5848225841198\n",
      "episode: 200 episode reward: 782.2820424666534 eps: 0.7360471625842407 avg reward (last 100): 828.7089903248419\n",
      "episode: 300 episode reward: 2953.227704527384 eps: 0.6659696926115485 avg reward (last 100): 606.213223968078\n",
      "episode: 400 episode reward: 786.5922121084441 eps: 0.6025641480906593 avg reward (last 100): 952.7932859256271\n",
      "episode: 500 episode reward: 2529.2912494042685 eps: 0.545195309324691 avg reward (last 100): -20.567482738701795\n",
      "episode: 600 episode reward: 2550.6746827556017 eps: 0.49328843452021 avg reward (last 100): 1246.413503511906\n",
      "episode: 700 episode reward: 5247.6841078254765 eps: 0.44632350181590114 avg reward (last 100): 1777.06308106429\n",
      "episode: 800 episode reward: -3243.5103704830926 eps: 0.4038299995153185 avg reward (last 100): 1246.756772293951\n",
      "episode: 900 episode reward: 186.80414359418683 eps: 0.3653822123303929 avg reward (last 100): 976.3664048688896\n",
      "episode: 1000 episode reward: 1726.4021208290742 eps: 0.33059495641157327 avg reward (last 100): 1464.8849277082522\n",
      "episode: 1100 episode reward: -3778.6872127681145 eps: 0.29911972043659035 avg reward (last 100): 1228.0336174689758\n",
      "episode: 1200 episode reward: 6728.887185111998 eps: 0.27064117409787486 avg reward (last 100): 1127.2292992321352\n",
      "episode: 1300 episode reward: -5135.675230165031 eps: 0.24487400900939155 avg reward (last 100): 646.9277093596505\n",
      "episode: 1400 episode reward: -5899.219406818451 eps: 0.22156008038394912 avg reward (last 100): 1507.3820744187856\n",
      "episode: 1500 episode reward: -904.1900853341904 eps: 0.2004658208452793 avg reward (last 100): 1948.5566594216698\n",
      "episode: 1600 episode reward: 196.87754707730164 eps: 0.1813799004655124 avg reward (last 100): 1657.423114509915\n",
      "episode: 1700 episode reward: 1219.5498401449404 eps: 0.16411110958546163 avg reward (last 100): 1670.2470611803324\n",
      "episode: 1800 episode reward: 323.34317052155893 eps: 0.14848644320704313 avg reward (last 100): 1545.3550215904088\n",
      "episode: 1900 episode reward: 1688.5847795314094 eps: 0.13434936776657827 avg reward (last 100): 1428.9877548485324\n",
      "episode: 2000 episode reward: 3316.1670001196326 eps: 0.12155825292489168 avg reward (last 100): 1483.375841417295\n",
      "episode: 2100 episode reward: 3271.8883650072785 eps: 0.109984952663304 avg reward (last 100): 337.9917834399328\n",
      "episode: 2200 episode reward: -2064.766836885634 eps: 0.1 avg reward (last 100): 1389.3440403841107\n",
      "episode: 2300 episode reward: 94.1566252292414 eps: 0.1 avg reward (last 100): 1147.7782885538315\n",
      "episode: 2400 episode reward: 3172.655740246939 eps: 0.1 avg reward (last 100): 1422.3790000914173\n",
      "episode: 2500 episode reward: 1431.6282860739793 eps: 0.1 avg reward (last 100): 1417.4485404011846\n",
      "episode: 2600 episode reward: -746.7964463813787 eps: 0.1 avg reward (last 100): 1227.1900304051808\n",
      "episode: 2700 episode reward: 5780.1728911232585 eps: 0.1 avg reward (last 100): 1245.1022983306414\n",
      "episode: 2800 episode reward: -3396.981860143562 eps: 0.1 avg reward (last 100): 1156.482969574347\n",
      "episode: 2900 episode reward: 3894.5952206189904 eps: 0.1 avg reward (last 100): 335.84745023199474\n",
      "episode: 3000 episode reward: 1554.7543616477433 eps: 0.1 avg reward (last 100): 1731.9468199771663\n",
      "episode: 3100 episode reward: 1766.5187794775557 eps: 0.1 avg reward (last 100): 846.2710109574207\n",
      "episode: 3200 episode reward: 1979.9115560716891 eps: 0.1 avg reward (last 100): 1594.163132004188\n",
      "episode: 3300 episode reward: 722.8388837182101 eps: 0.1 avg reward (last 100): 1338.3809562995355\n",
      "episode: 3400 episode reward: 6317.000392306187 eps: 0.1 avg reward (last 100): 1143.5692169065358\n",
      "episode: 3500 episode reward: 2776.3789342471027 eps: 0.1 avg reward (last 100): 1466.8228014379185\n",
      "episode: 3600 episode reward: 2623.8017521263573 eps: 0.1 avg reward (last 100): 850.122181352733\n",
      "episode: 3700 episode reward: -727.8350829605297 eps: 0.1 avg reward (last 100): 781.2925288083607\n",
      "episode: 3800 episode reward: 3321.907025138895 eps: 0.1 avg reward (last 100): 1211.4165010633726\n",
      "episode: 3900 episode reward: 574.2419158826779 eps: 0.1 avg reward (last 100): 1132.347686697134\n",
      "episode: 4000 episode reward: 2456.3482690248857 eps: 0.1 avg reward (last 100): 1436.777106794413\n",
      "episode: 4100 episode reward: -3465.6188064646967 eps: 0.1 avg reward (last 100): 1233.1191586569744\n",
      "episode: 4200 episode reward: 6799.507033387232 eps: 0.1 avg reward (last 100): 900.5178508467861\n",
      "episode: 4300 episode reward: 2178.521374883274 eps: 0.1 avg reward (last 100): 1605.4611668954801\n",
      "episode: 4400 episode reward: 12034.965931253279 eps: 0.1 avg reward (last 100): 1049.357367838863\n",
      "episode: 4500 episode reward: -2385.4745519686912 eps: 0.1 avg reward (last 100): 1851.9858317530363\n",
      "episode: 4600 episode reward: -2443.631069632849 eps: 0.1 avg reward (last 100): 1439.0684156099842\n",
      "episode: 4700 episode reward: -6603.614381078828 eps: 0.1 avg reward (last 100): 1392.3550136369909\n",
      "episode: 4800 episode reward: 3766.8933121394875 eps: 0.1 avg reward (last 100): 1235.3440103301812\n",
      "episode: 4900 episode reward: 6115.263528162301 eps: 0.1 avg reward (last 100): 1160.1526802063695\n",
      "episode: 5000 episode reward: 2348.430447059358 eps: 0.1 avg reward (last 100): 1029.698487907436\n",
      "episode: 5100 episode reward: -364.6379113167786 eps: 0.1 avg reward (last 100): 1250.736090779373\n",
      "episode: 5200 episode reward: 2929.1015393959497 eps: 0.1 avg reward (last 100): 1590.1035405060138\n",
      "episode: 5300 episode reward: 739.4756006551161 eps: 0.1 avg reward (last 100): 1987.8895689177054\n",
      "episode: 5400 episode reward: -6883.060049276044 eps: 0.1 avg reward (last 100): 1019.6714643347257\n",
      "episode: 5500 episode reward: -1214.3106262969613 eps: 0.1 avg reward (last 100): 1388.3395775041922\n",
      "episode: 5600 episode reward: 2150.624262193174 eps: 0.1 avg reward (last 100): 1530.2920279314224\n",
      "episode: 5700 episode reward: 11683.309747547053 eps: 0.1 avg reward (last 100): 990.0732020827974\n",
      "episode: 5800 episode reward: -3583.61200418706 eps: 0.1 avg reward (last 100): 1476.9716541390262\n",
      "episode: 5900 episode reward: 1620.468846561902 eps: 0.1 avg reward (last 100): 1431.0511780567413\n",
      "episode: 6000 episode reward: 1625.5024899681302 eps: 0.1 avg reward (last 100): 1004.6256711821528\n",
      "episode: 6100 episode reward: 2111.389155923549 eps: 0.1 avg reward (last 100): 927.1701490876007\n",
      "episode: 6200 episode reward: 2041.2137240089924 eps: 0.1 avg reward (last 100): 911.749925298025\n",
      "episode: 6300 episode reward: -1080.9040940262294 eps: 0.1 avg reward (last 100): 1168.2693737595937\n",
      "episode: 6400 episode reward: -2968.86794043205 eps: 0.1 avg reward (last 100): 1258.0758509575066\n",
      "episode: 6500 episode reward: 3954.2596038616557 eps: 0.1 avg reward (last 100): 1134.8207496219359\n",
      "episode: 6600 episode reward: 3979.2321967543885 eps: 0.1 avg reward (last 100): 1397.6236507324556\n",
      "episode: 6700 episode reward: 2617.6711870094878 eps: 0.1 avg reward (last 100): 1268.5348760239833\n",
      "episode: 6800 episode reward: 2358.404938629499 eps: 0.1 avg reward (last 100): 1697.0790365841217\n",
      "episode: 6900 episode reward: 4079.9311617729127 eps: 0.1 avg reward (last 100): 1037.2432065814032\n",
      "episode: 7000 episode reward: 2086.390471091474 eps: 0.1 avg reward (last 100): 1739.7543775843642\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-75f0bd8c393a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_games\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-75f0bd8c393a>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#         env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# observations is actually a single \"state\" ie past 5 days\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# TODO: REMOVE THIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprev_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\networks\\DQN.py\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, states, epsilon)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m#             return np.argmax(self.predict(np.atleast_3d(states))[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    977\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10353\u001b[0m         \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10354\u001b[0m         \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"new_axis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10355\u001b[1;33m         \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[0;32m  10356\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
