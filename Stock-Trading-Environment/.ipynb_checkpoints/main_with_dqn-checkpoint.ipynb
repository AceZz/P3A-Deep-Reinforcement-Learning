{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_action(action):\n",
    "    \"\"\"TEMPORARY FUNCTION, NEED TO GO TO CONTINUOUS ACTIONS\"\"\"\n",
    "    if action == 0:\n",
    "        return np.array([0, 0.2]) #buy stocks with 20% of remaining balance\n",
    "    elif action == 1:\n",
    "        return np.array([1, 0.2]) #sell 20% of stocks\n",
    "    elif action == 2:\n",
    "        return np.array([2, 0]) #do nothing\n",
    "\n",
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        action = convert_action(action)                # TODO: REMOVE THIS\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "#         rewards += reward\n",
    "        rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# def make_video(env, TrainNet):\n",
    "#     env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "#     rewards = 0\n",
    "#     steps = 0\n",
    "#     done = False\n",
    "#     observation = env.reset()\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = TrainNet.get_action(observation, 0)\n",
    "#         action = convert_action(action)                # TODO: REMOVE THIS\n",
    "#         observation, reward, done, _ = env.step(action)\n",
    "#         steps += 1\n",
    "#         rewards += reward\n",
    "#     print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('./data/AAPL.csv')\n",
    "    df = df.sort_values('Date')\n",
    "    env = StockTradingEnv(df)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 3                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    batch_size = 32\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N = 50000\n",
    "    total_rewards = np.empty(N)\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the account balance at the end of the episode (to be modified?)\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n",
      "episode: 0 episode reward: 9293.872599198712 eps: 0.8991 avg reward (last 100): 9293.872599198712\n",
      "episode: 100 episode reward: 7790.962653927165 eps: 0.8134986194699355 avg reward (last 100): 6455.067756212564\n",
      "episode: 200 episode reward: 9559.663757287795 eps: 0.7360471625842407 avg reward (last 100): 6628.896529084942\n",
      "episode: 300 episode reward: 5192.253883819036 eps: 0.6659696926115485 avg reward (last 100): 7154.836605550541\n",
      "episode: 400 episode reward: 6868.394542723617 eps: 0.6025641480906593 avg reward (last 100): 7450.39254053613\n",
      "episode: 500 episode reward: 8765.058201603657 eps: 0.545195309324691 avg reward (last 100): 7358.391863361836\n",
      "episode: 600 episode reward: 9311.277208006952 eps: 0.49328843452021 avg reward (last 100): 7794.671302325532\n",
      "episode: 700 episode reward: 8363.5042000042 eps: 0.44632350181590114 avg reward (last 100): 7782.257526415637\n",
      "episode: 800 episode reward: 8577.705031559219 eps: 0.4038299995153185 avg reward (last 100): 7927.142794506156\n",
      "episode: 900 episode reward: 7867.262228154407 eps: 0.3653822123303929 avg reward (last 100): 8126.678755534436\n",
      "episode: 1000 episode reward: 10055.634559227237 eps: 0.33059495641157327 avg reward (last 100): 8215.713643473693\n",
      "episode: 1100 episode reward: 7623.544432935079 eps: 0.29911972043659035 avg reward (last 100): 8387.593249142583\n",
      "episode: 1200 episode reward: 8413.840322663395 eps: 0.27064117409787486 avg reward (last 100): 8595.083192794242\n",
      "episode: 1300 episode reward: 9416.649513707143 eps: 0.24487400900939155 avg reward (last 100): 8668.818022555215\n",
      "episode: 1400 episode reward: 9865.506424311367 eps: 0.22156008038394912 avg reward (last 100): 8662.466422317508\n",
      "episode: 1500 episode reward: 5517.108553385605 eps: 0.2004658208452793 avg reward (last 100): 8899.963402682664\n",
      "episode: 1600 episode reward: 9319.220387094703 eps: 0.1813799004655124 avg reward (last 100): 8926.62664156066\n",
      "episode: 1700 episode reward: 9653.374543841195 eps: 0.16411110958546163 avg reward (last 100): 9023.95505712577\n",
      "episode: 1800 episode reward: 9658.655721607758 eps: 0.14848644320704313 avg reward (last 100): 8984.56009601841\n",
      "episode: 1900 episode reward: 9989.645889895897 eps: 0.13434936776657827 avg reward (last 100): 8986.89507828936\n",
      "episode: 2000 episode reward: 10100.275019766752 eps: 0.12155825292489168 avg reward (last 100): 9068.303639650303\n",
      "episode: 2100 episode reward: 8923.80536561324 eps: 0.109984952663304 avg reward (last 100): 9135.450464824835\n",
      "episode: 2200 episode reward: 10165.508029185077 eps: 0.1 avg reward (last 100): 9037.612049577716\n",
      "episode: 2300 episode reward: 9672.639840349124 eps: 0.1 avg reward (last 100): 9128.323542352202\n",
      "episode: 2400 episode reward: 8793.473523863731 eps: 0.1 avg reward (last 100): 9110.322857046294\n",
      "episode: 2500 episode reward: 9277.304652900315 eps: 0.1 avg reward (last 100): 9000.04006839172\n",
      "episode: 2600 episode reward: 7999.1918017595035 eps: 0.1 avg reward (last 100): 9089.119180075619\n",
      "episode: 2700 episode reward: 9952.887622268994 eps: 0.1 avg reward (last 100): 9106.00568481431\n",
      "episode: 2800 episode reward: 9477.260439194964 eps: 0.1 avg reward (last 100): 9101.991653949128\n",
      "episode: 2900 episode reward: 9485.896037357636 eps: 0.1 avg reward (last 100): 9105.462330859076\n",
      "episode: 3000 episode reward: 10000.0 eps: 0.1 avg reward (last 100): 8935.935596130123\n",
      "episode: 3100 episode reward: 9604.90786354842 eps: 0.1 avg reward (last 100): 9091.432091312012\n",
      "episode: 3200 episode reward: 6209.2336981463695 eps: 0.1 avg reward (last 100): 9079.823581361132\n",
      "episode: 3300 episode reward: 9282.842685505108 eps: 0.1 avg reward (last 100): 9158.195453160135\n",
      "episode: 3400 episode reward: 9915.142793889168 eps: 0.1 avg reward (last 100): 9172.896868274865\n",
      "episode: 3500 episode reward: 9511.06966952647 eps: 0.1 avg reward (last 100): 9204.175213567705\n",
      "episode: 3600 episode reward: 6654.3752390786885 eps: 0.1 avg reward (last 100): 9095.611878770915\n",
      "episode: 3700 episode reward: 10000.0 eps: 0.1 avg reward (last 100): 9176.374685069397\n",
      "episode: 3800 episode reward: 8527.925633768313 eps: 0.1 avg reward (last 100): 8979.39452526581\n",
      "episode: 3900 episode reward: 8204.67940101523 eps: 0.1 avg reward (last 100): 9045.496579065291\n",
      "episode: 4000 episode reward: 9706.08014531312 eps: 0.1 avg reward (last 100): 9221.92186060744\n",
      "episode: 4100 episode reward: 9281.990701153163 eps: 0.1 avg reward (last 100): 9115.904675747133\n",
      "episode: 4200 episode reward: 9854.385942357601 eps: 0.1 avg reward (last 100): 9196.882665058867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-140f3bacb349>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-140f3bacb349>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# TODO: REMOVE THIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprev_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m#         rewards += reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_worth\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py\u001b[0m in \u001b[0;36m_next_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m                         5, 'Close'].values / MAX_SHARE_PRICE,\n\u001b[0;32m     46\u001b[0m             self.df.loc[self.current_step: self.current_step +\n\u001b[1;32m---> 47\u001b[1;33m                         5, 'Volume'].values / MAX_NUM_SHARES,\n\u001b[0m\u001b[0;32m     48\u001b[0m         ])\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/AAPL.csv')\n",
    "df = df.sort_values('Date')\n",
    "env = StockTradingEnv(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
