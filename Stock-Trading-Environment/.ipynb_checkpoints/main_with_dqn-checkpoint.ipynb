{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN\n",
    "from utils.utils import convert_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/IBM_train.csv')\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "\n",
    "N_games = 50000  # number of training games\n",
    "N_save = 200    # interval between save model, must be over 200\n",
    "model_name = \"DQN\"\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "hidden_units = [64,128,256,256,128,64]\n",
    "in_log = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = convert_action(action, binary_action=True)\n",
    "        \n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "#         rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "def main():\n",
    "    env = StockTradingEnv(df, in_log=in_log)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 2                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    max_experiences = 1000\n",
    "    min_experiences = 25\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N_games = 50000\n",
    "    total_rewards = np.empty(N_games)\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N_games):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "        \n",
    "        # Save the model\n",
    "        if n % N_save == 0 and n>=N_save:\n",
    "            TrainNet.model.save_weights('save_models/{}_{}'.format(model_name, n), save_format='tf')\n",
    "\n",
    "        \n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the difference of net_worth between the beginning and the end of the step\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n",
      "episode: 0 episode reward: 413.36702614237583 eps: 0.8991 avg reward (last 100): 413.36702614237583\n",
      "episode: 100 episode reward: -388.01005538466416 eps: 0.8134986194699355 avg reward (last 100): 45.35134676999995\n",
      "episode: 200 episode reward: 2117.2697357163615 eps: 0.7360471625842407 avg reward (last 100): 114.02759442949774\n",
      "episode: 300 episode reward: 418.0577762283283 eps: 0.6659696926115485 avg reward (last 100): 222.05734880511446\n",
      "episode: 400 episode reward: 284.53334459006874 eps: 0.6025641480906593 avg reward (last 100): 184.49326269382925\n",
      "episode: 500 episode reward: -1109.992092843322 eps: 0.545195309324691 avg reward (last 100): 192.63011044984685\n",
      "episode: 600 episode reward: -574.012869884109 eps: 0.49328843452021 avg reward (last 100): 179.8653335655948\n",
      "episode: 700 episode reward: 701.7781332631312 eps: 0.44632350181590114 avg reward (last 100): 131.0884505295743\n",
      "episode: 800 episode reward: 455.11358082789957 eps: 0.4038299995153185 avg reward (last 100): 224.76589422380974\n",
      "episode: 900 episode reward: -122.89086778360252 eps: 0.3653822123303929 avg reward (last 100): 163.823266902453\n",
      "episode: 1000 episode reward: 737.7383620124965 eps: 0.33059495641157327 avg reward (last 100): 131.49224308158128\n",
      "episode: 1100 episode reward: 1159.3025144199455 eps: 0.29911972043659035 avg reward (last 100): 83.29839385618483\n",
      "episode: 1200 episode reward: 68.03145649404905 eps: 0.27064117409787486 avg reward (last 100): 186.21286449357538\n",
      "episode: 1300 episode reward: -1605.0344371469218 eps: 0.24487400900939155 avg reward (last 100): 103.0541218544172\n",
      "episode: 1400 episode reward: -36.73387142185857 eps: 0.22156008038394912 avg reward (last 100): 235.7654216765596\n",
      "episode: 1500 episode reward: -416.1798443465559 eps: 0.2004658208452793 avg reward (last 100): 298.5897240083147\n",
      "episode: 1600 episode reward: 639.7736074079712 eps: 0.1813799004655124 avg reward (last 100): 106.74440121907543\n",
      "episode: 1700 episode reward: 1139.6861287915035 eps: 0.16411110958546163 avg reward (last 100): 243.80842501892283\n",
      "episode: 1800 episode reward: -898.4636430358387 eps: 0.14848644320704313 avg reward (last 100): 126.38246412986837\n",
      "episode: 1900 episode reward: 802.3730103663111 eps: 0.13434936776657827 avg reward (last 100): 87.79708590913087\n",
      "episode: 2000 episode reward: 1404.5186820254785 eps: 0.12155825292489168 avg reward (last 100): 210.1353016874533\n",
      "episode: 2100 episode reward: 525.9944080826299 eps: 0.109984952663304 avg reward (last 100): 312.39337717000075\n",
      "episode: 2200 episode reward: 739.057249276575 eps: 0.1 avg reward (last 100): 313.3762244690629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-447492ae053c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_games\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-447492ae053c>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprev_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m    \u001b[1;31m# sum of gain_net_worth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#         rewards = reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_worth\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py\u001b[0m in \u001b[0;36m_next_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m                             5, 'Close'].values,\n\u001b[0;32m     80\u001b[0m                 self.df.loc[self.current_step: self.current_step +\n\u001b[1;32m---> 81\u001b[1;33m                             5, 'Volume'].values,\n\u001b[0m\u001b[0;32m     82\u001b[0m             ])\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
