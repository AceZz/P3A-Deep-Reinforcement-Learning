{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN\n",
    "from utils.utils import convert_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "N_games = 50000  # number of training games\n",
    "N_save = 200    # interval between save model, must be over 200\n",
    "model_name = \"DQN\"\n",
    "\n",
    "name_csv = './data/AAPL_train.csv'\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = convert_action(action)                # TODO: REMOVE THIS\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "#         rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# def make_video(env, TrainNet):\n",
    "#     env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "#     rewards = 0\n",
    "#     steps = 0\n",
    "#     done = False\n",
    "#     observation = env.reset()\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = TrainNet.get_action(observation, 0)\n",
    "#         action = convert_action(action)                # TODO: REMOVE THIS\n",
    "#         observation, reward, done, _ = env.step(action)\n",
    "#         steps += 1\n",
    "#         rewards += reward\n",
    "#     print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(name_csv)\n",
    "    df = df.sort_values('Date')\n",
    "    env = StockTradingEnv(df)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 3                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N_games = 50000\n",
    "    total_rewards = np.empty(N_games)\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N_games):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "        \n",
    "        # Save the model\n",
    "        if n % N_save == 0 and n>=N_save:\n",
    "            TrainNet.model.save_weights('save_models/{}_{}'.format(model_name, n), save_format='tf')\n",
    "\n",
    "        \n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the difference of net_worth between the beginning and the end of the step\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n",
      "episode: 0 episode reward: -251.82725592011957 eps: 0.8991 avg reward (last 100): -251.82725592011957\n",
      "episode: 100 episode reward: 2083.553473490274 eps: 0.8134986194699355 avg reward (last 100): 626.5101051531195\n",
      "episode: 200 episode reward: -251.49441494756684 eps: 0.7360471625842407 avg reward (last 100): 571.3676966939755\n",
      "episode: 300 episode reward: 904.2563119945225 eps: 0.6659696926115485 avg reward (last 100): 1116.317607238275\n",
      "episode: 400 episode reward: 1671.820955769177 eps: 0.6025641480906593 avg reward (last 100): 468.6749438507777\n",
      "episode: 500 episode reward: -1266.5563257438262 eps: 0.545195309324691 avg reward (last 100): 352.5839682251888\n",
      "episode: 600 episode reward: 690.1525370967684 eps: 0.49328843452021 avg reward (last 100): 1068.083830203361\n",
      "episode: 700 episode reward: 2893.263511673362 eps: 0.44632350181590114 avg reward (last 100): 730.6429715637545\n",
      "episode: 800 episode reward: 2891.652823270506 eps: 0.4038299995153185 avg reward (last 100): 656.7065199276607\n",
      "episode: 900 episode reward: -109.35000116618903 eps: 0.3653822123303929 avg reward (last 100): 1120.5966172555682\n",
      "episode: 1000 episode reward: 3873.260057824913 eps: 0.33059495641157327 avg reward (last 100): 910.7593991553235\n",
      "episode: 1100 episode reward: 1721.6031243239977 eps: 0.29911972043659035 avg reward (last 100): 1067.3513294473717\n",
      "episode: 1200 episode reward: 70.02679996726874 eps: 0.27064117409787486 avg reward (last 100): 523.4729115331961\n",
      "episode: 1300 episode reward: -2225.7996315925193 eps: 0.24487400900939155 avg reward (last 100): 1165.4921071641988\n",
      "episode: 1400 episode reward: 4510.402461371294 eps: 0.22156008038394912 avg reward (last 100): 615.0370628120731\n",
      "episode: 1500 episode reward: -2212.255733168334 eps: 0.2004658208452793 avg reward (last 100): 901.5450697784049\n",
      "episode: 1600 episode reward: -370.0043652940167 eps: 0.1813799004655124 avg reward (last 100): 820.902084268866\n",
      "episode: 1700 episode reward: 974.3422447131452 eps: 0.16411110958546163 avg reward (last 100): 580.6436713857847\n",
      "episode: 1800 episode reward: -1041.1018573397541 eps: 0.14848644320704313 avg reward (last 100): 783.4915180071197\n",
      "episode: 1900 episode reward: -4054.1738622532466 eps: 0.13434936776657827 avg reward (last 100): 520.8324983439097\n",
      "episode: 2000 episode reward: 539.3888686294358 eps: 0.12155825292489168 avg reward (last 100): 762.2352138256613\n",
      "episode: 2100 episode reward: 1659.265353958568 eps: 0.109984952663304 avg reward (last 100): 917.3092832695665\n",
      "episode: 2200 episode reward: 1685.470685519609 eps: 0.1 avg reward (last 100): 986.3004218056109\n",
      "episode: 2300 episode reward: 91.57106111332905 eps: 0.1 avg reward (last 100): 1009.8182510848542\n",
      "episode: 2400 episode reward: 663.7357185251585 eps: 0.1 avg reward (last 100): 1032.310698930901\n",
      "episode: 2500 episode reward: -332.6974714645803 eps: 0.1 avg reward (last 100): 827.1377855503193\n",
      "episode: 2600 episode reward: 4996.880870425975 eps: 0.1 avg reward (last 100): 940.4817561438725\n",
      "episode: 2700 episode reward: 2794.162256707359 eps: 0.1 avg reward (last 100): 1114.3616115412083\n",
      "episode: 2800 episode reward: 2037.8295803860747 eps: 0.1 avg reward (last 100): 534.7764670932862\n",
      "episode: 2900 episode reward: 932.7575532294031 eps: 0.1 avg reward (last 100): 913.9284618575907\n",
      "episode: 3000 episode reward: -877.6931473195473 eps: 0.1 avg reward (last 100): 1227.729702577788\n",
      "episode: 3100 episode reward: -424.48756852809856 eps: 0.1 avg reward (last 100): 1162.8618932290656\n",
      "episode: 3200 episode reward: 3770.185438650922 eps: 0.1 avg reward (last 100): 1105.664820028608\n",
      "episode: 3300 episode reward: 470.3931618982497 eps: 0.1 avg reward (last 100): 911.5894559450912\n",
      "episode: 3400 episode reward: 1622.5191319193636 eps: 0.1 avg reward (last 100): 846.9435738917108\n",
      "episode: 3500 episode reward: -2634.875031268788 eps: 0.1 avg reward (last 100): 1237.5803581305902\n",
      "episode: 3600 episode reward: 798.8743667533981 eps: 0.1 avg reward (last 100): 739.8215910406553\n",
      "episode: 3700 episode reward: 4373.6862750261425 eps: 0.1 avg reward (last 100): 999.1816999165936\n",
      "episode: 3800 episode reward: 478.940565440942 eps: 0.1 avg reward (last 100): 951.7968004687008\n",
      "episode: 3900 episode reward: -3182.4295907896594 eps: 0.1 avg reward (last 100): 691.706602583052\n",
      "episode: 4000 episode reward: 1946.3522078200986 eps: 0.1 avg reward (last 100): 986.4968278064753\n",
      "episode: 4100 episode reward: 1662.683524953376 eps: 0.1 avg reward (last 100): 1105.2177353839252\n",
      "episode: 4200 episode reward: 3777.7540962691783 eps: 0.1 avg reward (last 100): 1051.0224433338162\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
