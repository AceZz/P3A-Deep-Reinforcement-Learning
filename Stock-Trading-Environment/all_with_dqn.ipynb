{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN\n",
    "from utils.utils import convert_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/IBM_train.csv')\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "N_games = 101  # number of training games\n",
    "N_save = 200    # interval between save model, must be over 200\n",
    "model_name = \"DQN\"\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "hidden_units = [64,128,256,256,128,64]\n",
    "in_log = True\n",
    "\n",
    "N_agent = 1 #look for N_agent agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = convert_action(action, binary_action=True)\n",
    "        \n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "#         rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "def main():\n",
    "    max_net_worth = 0\n",
    "    for k in range(N_agent):\n",
    "        env = StockTradingEnv(df, in_log=in_log)\n",
    "        gamma = 0.99\n",
    "        copy_step = 25\n",
    "    #     num_states = len(env.observation_space.sample())\n",
    "        input_shape = env.observation_space.sample().shape\n",
    "    #     num_actions = env.action_space.n\n",
    "        num_actions = 2                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "        max_experiences = 1000\n",
    "        min_experiences = 25\n",
    "        lr = 1e-2\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        log_dir = 'logs/dqn/' + current_time\n",
    "        summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "        TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "        print(\"\\r DeepQ Networks successfully created\",end=\"\")\n",
    "        total_rewards = np.empty(N_games)\n",
    "        epsilon = 0.9\n",
    "        decay = 0.99\n",
    "        min_epsilon = 0.1\n",
    "        print(\"\\r Starting training...\",end=\"\")\n",
    "        for n in range(N_games):\n",
    "            epsilon = max(min_epsilon, epsilon * decay)\n",
    "            total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "            total_rewards[n] = total_reward\n",
    "            avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "                tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "            if n % 20 == 0:\n",
    "                print(\"\\r episode:\", n, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,end=\"\")\n",
    "\n",
    "            # Save the model\n",
    "    #             if n % N_save == 0 and n>=N_save:\n",
    "    #                 TrainNet.model.save_weights('save_models/{}_{}'.format(model_name, n), save_format='tf')\n",
    "\n",
    "        env.close()\n",
    "\n",
    "\n",
    "        ### val part\n",
    "        df_val = pd.read_csv('./data/IBM_val.csv')\n",
    "\n",
    "        env = StockTradingEnv(df_val)\n",
    "        input_shape = env.observation_space.sample().shape\n",
    "        observation = env.reset_to_day_one().reshape(1,input_shape[0],input_shape[1]).astype('float32') # necessary to reshape each observation\n",
    "\n",
    "        for i in range(len(df_val.loc[:, 'Open'].values) - 6):\n",
    "            prediction = TrainNet.model.predict(observation.reshape(1,input_shape[0],input_shape[1]).astype('float32'))\n",
    "            action = np.argmax(prediction[0])\n",
    "            action = convert_action(action, binary_action=True)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        net_worth = env.net_worth\n",
    "\n",
    "        if net_worth > max_net_worth:\n",
    "            max_net_worth = net_worth\n",
    "            TrainNet.model.save_weights('save_models/{}_best'.format(model_name), save_format='tf')\n",
    "\n",
    "        print(\"\\r agent {} of {}, net_worth: {}, max_net_worth: {}\".format(k, N_agent, net_worth, max_net_worth))\n",
    "\n",
    "        env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the difference of net_worth between the beginning and the end of the step\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      " episode: 0 eps: 0.891 avg reward (last 100): -1283.4953895031977"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}