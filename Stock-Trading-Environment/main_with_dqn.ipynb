{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_action(action):\n",
    "    \"\"\"TEMPORARY FUNCTION, NEED TO GO TO CONTINUOUS ACTIONS\"\"\"\n",
    "    if action == 0:\n",
    "        return np.array([0, 0.2]) #buy stocks with 20% of remaining balance\n",
    "    elif action == 1:\n",
    "        return np.array([1, 0.2]) #sell 20% of stocks\n",
    "    elif action == 2:\n",
    "        return np.array([2, 0]) #do nothing\n",
    "\n",
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        action = convert_action(action)                # TODO: REMOVE THIS\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "#         rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# def make_video(env, TrainNet):\n",
    "#     env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "#     rewards = 0\n",
    "#     steps = 0\n",
    "#     done = False\n",
    "#     observation = env.reset()\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = TrainNet.get_action(observation, 0)\n",
    "#         action = convert_action(action)                # TODO: REMOVE THIS\n",
    "#         observation, reward, done, _ = env.step(action)\n",
    "#         steps += 1\n",
    "#         rewards += reward\n",
    "#     print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('./data/AAPL.csv')\n",
    "    df = df.sort_values('Date')\n",
    "    env = StockTradingEnv(df)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 3                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    batch_size = 32\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N = 50000\n",
    "    total_rewards = np.empty(N)\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the difference of net_worth between the beginning and the end of the step\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n",
      "episode: 0 episode reward: -7226.149602027905 eps: 0.8991 avg reward (last 100): -7226.149602027905\n",
      "episode: 100 episode reward: 3641.698844763745 eps: 0.8134986194699355 avg reward (last 100): 607.2756762468525\n",
      "episode: 200 episode reward: 837.2523438835015 eps: 0.7360471625842407 avg reward (last 100): 731.5588355111578\n",
      "episode: 300 episode reward: 1337.741727524699 eps: 0.6659696926115485 avg reward (last 100): 958.6324740043328\n",
      "episode: 400 episode reward: -3039.3909420346945 eps: 0.6025641480906593 avg reward (last 100): 929.1525744936882\n",
      "episode: 500 episode reward: 389.4557084971493 eps: 0.545195309324691 avg reward (last 100): 734.5177287566529\n",
      "episode: 600 episode reward: 1423.247349829735 eps: 0.49328843452021 avg reward (last 100): 606.4515245065472\n",
      "episode: 700 episode reward: 2462.3172280292656 eps: 0.44632350181590114 avg reward (last 100): 573.869584065967\n",
      "episode: 800 episode reward: 4214.834465006526 eps: 0.4038299995153185 avg reward (last 100): 928.3240200031091\n",
      "episode: 900 episode reward: -617.8247817960055 eps: 0.3653822123303929 avg reward (last 100): 569.119121136796\n",
      "episode: 1000 episode reward: -1199.955992319381 eps: 0.33059495641157327 avg reward (last 100): 587.0183385438522\n",
      "episode: 1100 episode reward: -4369.272817783534 eps: 0.29911972043659035 avg reward (last 100): 635.3353976773881\n",
      "episode: 1200 episode reward: -1610.9362847653447 eps: 0.27064117409787486 avg reward (last 100): 634.7195445765\n",
      "episode: 1300 episode reward: 1553.0436401628685 eps: 0.24487400900939155 avg reward (last 100): 1184.7838641340975\n",
      "episode: 1400 episode reward: 6136.191151489562 eps: 0.22156008038394912 avg reward (last 100): 899.35205370108\n",
      "episode: 1500 episode reward: 4226.996305707844 eps: 0.2004658208452793 avg reward (last 100): 908.3746722968552\n",
      "episode: 1600 episode reward: 1100.134172817805 eps: 0.1813799004655124 avg reward (last 100): 1160.8636659403421\n",
      "episode: 1700 episode reward: 3037.905462478677 eps: 0.16411110958546163 avg reward (last 100): 881.5024146803356\n",
      "episode: 1800 episode reward: 1763.7931599140975 eps: 0.14848644320704313 avg reward (last 100): 1018.5626245470933\n",
      "episode: 1900 episode reward: 9128.429405644816 eps: 0.13434936776657827 avg reward (last 100): 613.306671369438\n",
      "episode: 2000 episode reward: 798.4248428339943 eps: 0.12155825292489168 avg reward (last 100): 347.02373798973576\n",
      "episode: 2100 episode reward: 1338.4323163788886 eps: 0.109984952663304 avg reward (last 100): 761.3132189973294\n",
      "episode: 2200 episode reward: 2574.2218214039185 eps: 0.1 avg reward (last 100): 1029.0454563461215\n",
      "episode: 2300 episode reward: -7818.637008413596 eps: 0.1 avg reward (last 100): 1047.5930606061524\n",
      "episode: 2400 episode reward: -1850.4399172735093 eps: 0.1 avg reward (last 100): 1509.7375734014702\n",
      "episode: 2500 episode reward: 1073.970708445353 eps: 0.1 avg reward (last 100): 1265.5306503040752\n",
      "episode: 2600 episode reward: 221.88823357621732 eps: 0.1 avg reward (last 100): 923.6902179037937\n",
      "episode: 2700 episode reward: -193.5224262967913 eps: 0.1 avg reward (last 100): 1031.9783414860717\n",
      "episode: 2800 episode reward: 4281.152445494687 eps: 0.1 avg reward (last 100): 532.3096356797956\n",
      "episode: 2900 episode reward: 6505.694458791197 eps: 0.1 avg reward (last 100): 1357.588948024975\n",
      "episode: 3000 episode reward: 6304.627734342772 eps: 0.1 avg reward (last 100): 580.9123708829499\n",
      "episode: 3100 episode reward: -2934.8933349335703 eps: 0.1 avg reward (last 100): 686.0890659236281\n",
      "episode: 3200 episode reward: 1587.2494466637818 eps: 0.1 avg reward (last 100): 1115.7627769353771\n",
      "episode: 3300 episode reward: 1021.0275011255544 eps: 0.1 avg reward (last 100): 742.4241953712728\n",
      "episode: 3400 episode reward: -839.3405515553477 eps: 0.1 avg reward (last 100): 781.426852635387\n",
      "episode: 3500 episode reward: 990.2124292143144 eps: 0.1 avg reward (last 100): 854.9374277715539\n",
      "episode: 3600 episode reward: -1194.4810354667152 eps: 0.1 avg reward (last 100): 601.422308170905\n",
      "episode: 3700 episode reward: -166.90321190247414 eps: 0.1 avg reward (last 100): 1301.2464348762535\n",
      "episode: 3800 episode reward: -32.789901429287056 eps: 0.1 avg reward (last 100): 1466.8281883221473\n",
      "episode: 3900 episode reward: 2461.0751616056095 eps: 0.1 avg reward (last 100): 884.9673067616235\n",
      "episode: 4000 episode reward: 2774.098615291583 eps: 0.1 avg reward (last 100): 990.9631634961755\n",
      "episode: 4100 episode reward: 2246.9177481130555 eps: 0.1 avg reward (last 100): 709.268173225343\n",
      "episode: 4200 episode reward: 1448.5520034041801 eps: 0.1 avg reward (last 100): 1170.3036562677623\n",
      "episode: 4300 episode reward: -1008.3915379375921 eps: 0.1 avg reward (last 100): 822.5917183578418\n",
      "episode: 4400 episode reward: 1172.6036768305748 eps: 0.1 avg reward (last 100): 514.4155450410918\n",
      "episode: 4500 episode reward: 717.2602855438181 eps: 0.1 avg reward (last 100): 1192.3701775626193\n",
      "episode: 4600 episode reward: 1293.832457920762 eps: 0.1 avg reward (last 100): 791.679021119154\n",
      "episode: 4700 episode reward: 3733.8387317407924 eps: 0.1 avg reward (last 100): 429.7520543189452\n",
      "episode: 4800 episode reward: 1795.176566795557 eps: 0.1 avg reward (last 100): 1049.3272646451144\n",
      "episode: 4900 episode reward: 5314.574542385299 eps: 0.1 avg reward (last 100): 610.9249466334942\n",
      "episode: 5000 episode reward: 103.69164940400515 eps: 0.1 avg reward (last 100): 1146.782853081099\n",
      "episode: 5100 episode reward: -4674.292888667398 eps: 0.1 avg reward (last 100): 1074.0144448128642\n",
      "episode: 5200 episode reward: 2014.5602965358012 eps: 0.1 avg reward (last 100): 1024.8907560767514\n",
      "episode: 5300 episode reward: 1462.8377769091112 eps: 0.1 avg reward (last 100): 904.7648446025177\n",
      "episode: 5400 episode reward: 6426.315806942035 eps: 0.1 avg reward (last 100): 1074.0999805248039\n",
      "episode: 5500 episode reward: 2303.3675163093903 eps: 0.1 avg reward (last 100): 1241.66251842932\n",
      "episode: 5600 episode reward: 1241.81013631938 eps: 0.1 avg reward (last 100): 1282.1646250659535\n",
      "episode: 5700 episode reward: 642.3967156605177 eps: 0.1 avg reward (last 100): 1401.0111222648356\n",
      "episode: 5800 episode reward: -4745.579297640381 eps: 0.1 avg reward (last 100): 972.7684807655345\n",
      "episode: 5900 episode reward: -3856.1121844130903 eps: 0.1 avg reward (last 100): 587.3867424493338\n",
      "episode: 6000 episode reward: -5234.876675193358 eps: 0.1 avg reward (last 100): 522.3148507986467\n",
      "episode: 6100 episode reward: 2679.594122536193 eps: 0.1 avg reward (last 100): 993.4128781927501\n",
      "episode: 6200 episode reward: 2439.3947576028913 eps: 0.1 avg reward (last 100): 963.5637676554907\n",
      "episode: 6300 episode reward: 204.31672823537883 eps: 0.1 avg reward (last 100): 1327.515730755293\n",
      "episode: 6400 episode reward: 827.8042983952255 eps: 0.1 avg reward (last 100): 692.028345730849\n",
      "episode: 6500 episode reward: -4442.823769005015 eps: 0.1 avg reward (last 100): 843.9327544182307\n",
      "episode: 6600 episode reward: 1445.7866324556762 eps: 0.1 avg reward (last 100): 806.2677307541708\n",
      "episode: 6700 episode reward: 1100.4670723398758 eps: 0.1 avg reward (last 100): 1550.6224600104445\n",
      "episode: 6800 episode reward: 9723.228086299241 eps: 0.1 avg reward (last 100): 829.1543736832976\n",
      "episode: 6900 episode reward: 4069.138974631891 eps: 0.1 avg reward (last 100): 836.8942619207628\n",
      "episode: 7000 episode reward: -4571.184083137379 eps: 0.1 avg reward (last 100): 835.3527961443439\n",
      "episode: 7100 episode reward: 1752.3210635948544 eps: 0.1 avg reward (last 100): 1319.2434565467156\n",
      "episode: 7200 episode reward: 2187.4045670486903 eps: 0.1 avg reward (last 100): 857.0926205755536\n",
      "episode: 7300 episode reward: -4390.7637412127615 eps: 0.1 avg reward (last 100): 713.6469636914662\n",
      "episode: 7400 episode reward: 4196.726227769013 eps: 0.1 avg reward (last 100): 956.1843901644314\n",
      "episode: 7500 episode reward: 939.8767289183925 eps: 0.1 avg reward (last 100): 1068.4551286391943\n",
      "episode: 7600 episode reward: 2047.2985082801006 eps: 0.1 avg reward (last 100): 1218.675986016419\n",
      "episode: 7700 episode reward: -1385.9118148150847 eps: 0.1 avg reward (last 100): 973.5750440024694\n",
      "episode: 7800 episode reward: 5567.315697317552 eps: 0.1 avg reward (last 100): 1304.9645535009872\n",
      "episode: 7900 episode reward: 6548.428512896466 eps: 0.1 avg reward (last 100): 1025.943797530775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8000 episode reward: 1695.7990112158568 eps: 0.1 avg reward (last 100): 637.6669147204427\n",
      "episode: 8100 episode reward: 1269.6738092414162 eps: 0.1 avg reward (last 100): 555.4478616016333\n",
      "episode: 8200 episode reward: 543.0899459567681 eps: 0.1 avg reward (last 100): 552.6372472089217\n",
      "episode: 8300 episode reward: 2924.2191316035387 eps: 0.1 avg reward (last 100): 1421.1819325132813\n",
      "episode: 8400 episode reward: 471.3626911206957 eps: 0.1 avg reward (last 100): 931.0706218169789\n",
      "episode: 8500 episode reward: 5768.747096651334 eps: 0.1 avg reward (last 100): 1875.6754281639085\n",
      "episode: 8600 episode reward: -76.1312811984244 eps: 0.1 avg reward (last 100): 824.4150020798189\n",
      "episode: 8700 episode reward: 737.1564747071516 eps: 0.1 avg reward (last 100): 546.757035094487\n",
      "episode: 8800 episode reward: 2681.354897326808 eps: 0.1 avg reward (last 100): 889.6128501368084\n",
      "episode: 8900 episode reward: -2343.655164599142 eps: 0.1 avg reward (last 100): 1116.6703096513004\n",
      "episode: 9000 episode reward: -6758.5396104875745 eps: 0.1 avg reward (last 100): 665.4424445650756\n",
      "episode: 9100 episode reward: 3858.0073420617227 eps: 0.1 avg reward (last 100): 930.5183130594932\n",
      "episode: 9200 episode reward: -1981.2082643946278 eps: 0.1 avg reward (last 100): 1061.8413181243764\n",
      "episode: 9300 episode reward: 1767.8258485641563 eps: 0.1 avg reward (last 100): 783.2695740877151\n",
      "episode: 9400 episode reward: 2314.6218752873338 eps: 0.1 avg reward (last 100): 909.9682777838478\n",
      "episode: 9500 episode reward: 1653.5422968974653 eps: 0.1 avg reward (last 100): 1232.8228131355252\n",
      "episode: 9600 episode reward: 2243.1597592964154 eps: 0.1 avg reward (last 100): 1328.7694188410453\n",
      "episode: 9700 episode reward: 972.9275245285789 eps: 0.1 avg reward (last 100): 1428.6679458294884\n",
      "episode: 9800 episode reward: 773.5577974495427 eps: 0.1 avg reward (last 100): 1027.2447088994707\n",
      "episode: 9900 episode reward: 5176.548385846838 eps: 0.1 avg reward (last 100): 746.5033782147442\n",
      "episode: 10000 episode reward: 4111.423231714256 eps: 0.1 avg reward (last 100): 1243.945971194828\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-5a8261d5b121>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5a8261d5b121>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# TODO: REMOVE THIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprev_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m    \u001b[1;31m# sum of gain_net_worth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m#         rewards = reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_worth\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py\u001b[0m in \u001b[0;36m_next_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m                         5, 'Close'].values / MAX_SHARE_PRICE,\n\u001b[0;32m     46\u001b[0m             self.df.loc[self.current_step: self.current_step +\n\u001b[1;32m---> 47\u001b[1;33m                         5, 'Volume'].values / MAX_NUM_SHARES,\n\u001b[0m\u001b[0;32m     48\u001b[0m         ])\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/AAPL.csv')\n",
    "df = df.sort_values('Date')\n",
    "env = StockTradingEnv(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
