{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN\n",
    "from utils.utils import convert_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "N_games = 50000  # number of training games\n",
    "N_save = 500    # interval between save model, must be over 200\n",
    "model_name = \"DQN\"\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = convert_action(action)                # TODO: REMOVE THIS\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "#         rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# def make_video(env, TrainNet):\n",
    "#     env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "#     rewards = 0\n",
    "#     steps = 0\n",
    "#     done = False\n",
    "#     observation = env.reset()\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = TrainNet.get_action(observation, 0)\n",
    "#         action = convert_action(action)                # TODO: REMOVE THIS\n",
    "#         observation, reward, done, _ = env.step(action)\n",
    "#         steps += 1\n",
    "#         rewards += reward\n",
    "#     print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('./data/AAPL.csv')\n",
    "    df = df.sort_values('Date')\n",
    "    env = StockTradingEnv(df)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 3                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N_games = 50000\n",
    "    total_rewards = np.empty(N_games)\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N_games):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "        \n",
    "        # Save the model\n",
    "        if n % N_save == 0 and n>=N_save:\n",
    "            TrainNet.model.save_weights('save_models/{}_{}'.format(model_name, n), save_format='tf')\n",
    "\n",
    "        \n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the difference of net_worth between the beginning and the end of the step\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n",
      "episode: 0 episode reward: 2813.4527644554473 eps: 0.8991 avg reward (last 100): 2813.4527644554473\n",
      "episode: 100 episode reward: 279.39175076859465 eps: 0.8134986194699355 avg reward (last 100): 121.96589311118454\n",
      "episode: 200 episode reward: 1964.8893389911063 eps: 0.7360471625842407 avg reward (last 100): 590.8715666568311\n",
      "episode: 300 episode reward: 419.1370444178556 eps: 0.6659696926115485 avg reward (last 100): 448.7729660054564\n",
      "episode: 400 episode reward: 2329.236702562979 eps: 0.6025641480906593 avg reward (last 100): 899.2924588026743\n",
      "episode: 500 episode reward: 849.2114416451805 eps: 0.545195309324691 avg reward (last 100): 539.9068127462876\n",
      "episode: 600 episode reward: 484.07331987522593 eps: 0.49328843452021 avg reward (last 100): 761.4236627514086\n",
      "episode: 700 episode reward: 3497.1657938156113 eps: 0.44632350181590114 avg reward (last 100): 1057.5417690293111\n",
      "episode: 800 episode reward: -1107.6033283439701 eps: 0.4038299995153185 avg reward (last 100): 821.6856949960445\n",
      "episode: 900 episode reward: -267.6575216807141 eps: 0.3653822123303929 avg reward (last 100): 432.40622541160855\n",
      "episode: 1000 episode reward: 754.5534846330083 eps: 0.33059495641157327 avg reward (last 100): 613.0169621802544\n",
      "episode: 1100 episode reward: 1512.9855649156743 eps: 0.29911972043659035 avg reward (last 100): 1017.9346905252735\n",
      "episode: 1200 episode reward: 2974.3732381848986 eps: 0.27064117409787486 avg reward (last 100): 833.97599848724\n",
      "episode: 1300 episode reward: 1999.4172867224715 eps: 0.24487400900939155 avg reward (last 100): 986.4930248971857\n",
      "episode: 1400 episode reward: 727.3192918361347 eps: 0.22156008038394912 avg reward (last 100): 800.4363160715238\n",
      "episode: 1500 episode reward: -764.6301166067751 eps: 0.2004658208452793 avg reward (last 100): 851.6794601129903\n",
      "episode: 1600 episode reward: 1578.1700414024617 eps: 0.1813799004655124 avg reward (last 100): 856.9597246027346\n",
      "episode: 1700 episode reward: 1461.136870748558 eps: 0.16411110958546163 avg reward (last 100): 900.3210825620375\n",
      "episode: 1800 episode reward: 2354.429379863892 eps: 0.14848644320704313 avg reward (last 100): 923.9267010633553\n",
      "episode: 1900 episode reward: 6464.108866037306 eps: 0.13434936776657827 avg reward (last 100): 1184.2510721903116\n",
      "episode: 2000 episode reward: -4183.124126728902 eps: 0.12155825292489168 avg reward (last 100): 507.4428421069452\n",
      "episode: 2100 episode reward: 1158.8348881478396 eps: 0.109984952663304 avg reward (last 100): 1003.9865529785997\n",
      "episode: 2200 episode reward: -3589.573734753163 eps: 0.1 avg reward (last 100): 1286.380622035743\n",
      "episode: 2300 episode reward: -2184.668375245139 eps: 0.1 avg reward (last 100): 881.5508078124734\n",
      "episode: 2400 episode reward: 2672.119930256469 eps: 0.1 avg reward (last 100): 1191.7189945472285\n",
      "episode: 2500 episode reward: 982.9777901761881 eps: 0.1 avg reward (last 100): 1093.7499669073445\n",
      "episode: 2600 episode reward: -1049.8699221657498 eps: 0.1 avg reward (last 100): 1319.352126713926\n",
      "episode: 2700 episode reward: -449.2860431897716 eps: 0.1 avg reward (last 100): 685.2212074097854\n",
      "episode: 2800 episode reward: -224.26637129985102 eps: 0.1 avg reward (last 100): 1228.8874759938258\n",
      "episode: 2900 episode reward: 836.7046350768996 eps: 0.1 avg reward (last 100): 1158.9247878904232\n",
      "episode: 3000 episode reward: -788.0341058476406 eps: 0.1 avg reward (last 100): 1384.7615107877236\n",
      "episode: 3100 episode reward: 915.4304508792338 eps: 0.1 avg reward (last 100): 931.5548616434006\n",
      "episode: 3200 episode reward: 2136.271501300169 eps: 0.1 avg reward (last 100): 1153.8520917521655\n",
      "episode: 3300 episode reward: -29.93268361510127 eps: 0.1 avg reward (last 100): 826.4251358564597\n",
      "episode: 3400 episode reward: -3966.79564306288 eps: 0.1 avg reward (last 100): 560.9193841100642\n",
      "episode: 3500 episode reward: -360.34030107097715 eps: 0.1 avg reward (last 100): 638.2639424313755\n",
      "episode: 3600 episode reward: -693.5385152429662 eps: 0.1 avg reward (last 100): 203.17134795730868\n",
      "episode: 3700 episode reward: 681.4121989872401 eps: 0.1 avg reward (last 100): 922.6337760520736\n",
      "episode: 3800 episode reward: 2125.2975659794283 eps: 0.1 avg reward (last 100): 765.1113820846907\n",
      "episode: 3900 episode reward: 1424.2052260632645 eps: 0.1 avg reward (last 100): 1000.3423932444286\n",
      "episode: 4000 episode reward: 683.9129909894891 eps: 0.1 avg reward (last 100): 1253.487761269332\n",
      "episode: 4100 episode reward: 2090.6861159793243 eps: 0.1 avg reward (last 100): 847.6106159718378\n",
      "episode: 4200 episode reward: 1926.1030186452808 eps: 0.1 avg reward (last 100): 502.599439630383\n",
      "episode: 4300 episode reward: -181.48771512207168 eps: 0.1 avg reward (last 100): 1327.1417236908433\n",
      "episode: 4400 episode reward: 3330.1754854550363 eps: 0.1 avg reward (last 100): 951.8448911829333\n",
      "episode: 4500 episode reward: -1154.6252995802424 eps: 0.1 avg reward (last 100): 450.3149622984021\n",
      "episode: 4600 episode reward: 647.6896983047955 eps: 0.1 avg reward (last 100): 631.113389802816\n",
      "episode: 4700 episode reward: 910.7939340502653 eps: 0.1 avg reward (last 100): 1522.3357687329922\n",
      "episode: 4800 episode reward: -2302.236263398535 eps: 0.1 avg reward (last 100): 916.2064784146436\n",
      "episode: 4900 episode reward: 782.6738528089827 eps: 0.1 avg reward (last 100): 454.74514915436623\n",
      "episode: 5000 episode reward: 5503.659435424088 eps: 0.1 avg reward (last 100): 786.3673088772558\n",
      "episode: 5100 episode reward: -134.23548205132283 eps: 0.1 avg reward (last 100): 1142.663097903291\n",
      "episode: 5200 episode reward: 815.9073552983718 eps: 0.1 avg reward (last 100): 1048.3399590762597\n",
      "episode: 5300 episode reward: 3220.5827042257715 eps: 0.1 avg reward (last 100): 562.3678385393341\n",
      "episode: 5400 episode reward: -2932.638281853192 eps: 0.1 avg reward (last 100): 1293.0401668131701\n",
      "episode: 5500 episode reward: 161.39531294281005 eps: 0.1 avg reward (last 100): 628.9364431091334\n",
      "episode: 5600 episode reward: 260.65457819557014 eps: 0.1 avg reward (last 100): 964.2373717851978\n",
      "episode: 5700 episode reward: 1861.3520389800797 eps: 0.1 avg reward (last 100): 979.2076773878209\n",
      "episode: 5800 episode reward: -386.3747409297357 eps: 0.1 avg reward (last 100): 1489.4471428774395\n",
      "episode: 5900 episode reward: -728.6397231522478 eps: 0.1 avg reward (last 100): 176.0439947420452\n",
      "episode: 6000 episode reward: 2607.5844551741648 eps: 0.1 avg reward (last 100): 1024.2466468721482\n",
      "episode: 6100 episode reward: 2366.6851780484885 eps: 0.1 avg reward (last 100): 1072.257364415355\n",
      "episode: 6200 episode reward: 3057.7102880660077 eps: 0.1 avg reward (last 100): 847.5485363287108\n",
      "episode: 6300 episode reward: 539.372957326148 eps: 0.1 avg reward (last 100): 1281.923918293636\n",
      "episode: 6400 episode reward: 300.20351048072916 eps: 0.1 avg reward (last 100): 770.3443232237641\n",
      "episode: 6500 episode reward: 900.1155339829165 eps: 0.1 avg reward (last 100): 995.9783257849147\n",
      "episode: 6600 episode reward: -1598.2123760534287 eps: 0.1 avg reward (last 100): 906.7337259652497\n",
      "episode: 6700 episode reward: -2377.957446576922 eps: 0.1 avg reward (last 100): 1125.6335664738322\n",
      "episode: 6800 episode reward: -338.805739363881 eps: 0.1 avg reward (last 100): 1076.3409738388175\n",
      "episode: 6900 episode reward: 1613.345754493619 eps: 0.1 avg reward (last 100): 880.7764040567266\n",
      "episode: 7000 episode reward: 568.0022827162793 eps: 0.1 avg reward (last 100): 1338.9730533687423\n",
      "episode: 7100 episode reward: 840.7494469050525 eps: 0.1 avg reward (last 100): 1034.5123509512284\n",
      "episode: 7200 episode reward: 412.0607590196105 eps: 0.1 avg reward (last 100): 939.913713026527\n",
      "episode: 7300 episode reward: 6094.876236420456 eps: 0.1 avg reward (last 100): 430.871003764297\n",
      "episode: 7400 episode reward: -2898.409247904063 eps: 0.1 avg reward (last 100): 754.5751599285398\n",
      "episode: 7500 episode reward: 4319.3694133556855 eps: 0.1 avg reward (last 100): 836.074947983294\n",
      "episode: 7600 episode reward: 3983.535588236744 eps: 0.1 avg reward (last 100): 1110.2429688341788\n",
      "episode: 7700 episode reward: -1121.8444437160852 eps: 0.1 avg reward (last 100): 865.5155146327054\n",
      "episode: 7800 episode reward: 1952.7704894142516 eps: 0.1 avg reward (last 100): 1697.987691773331\n",
      "episode: 7900 episode reward: 3756.202025770295 eps: 0.1 avg reward (last 100): 699.8875629681577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8000 episode reward: 11030.267438538867 eps: 0.1 avg reward (last 100): 537.9469444741176\n",
      "episode: 8100 episode reward: -216.1120004589029 eps: 0.1 avg reward (last 100): 1422.4088388178598\n",
      "episode: 8200 episode reward: -2664.4057586174977 eps: 0.1 avg reward (last 100): 1049.9611693725378\n",
      "episode: 8300 episode reward: 1770.6275948484763 eps: 0.1 avg reward (last 100): 1111.9869029096803\n",
      "episode: 8400 episode reward: -1396.9130426986649 eps: 0.1 avg reward (last 100): 1323.053693925063\n",
      "episode: 8500 episode reward: 4563.558645094567 eps: 0.1 avg reward (last 100): 766.2635645482775\n",
      "episode: 8600 episode reward: -8664.082215470087 eps: 0.1 avg reward (last 100): 1022.2737355374836\n",
      "episode: 8700 episode reward: 5432.11405272288 eps: 0.1 avg reward (last 100): 622.9898768573042\n",
      "episode: 8800 episode reward: 1963.1648731755395 eps: 0.1 avg reward (last 100): 1374.3225716345573\n",
      "episode: 8900 episode reward: -806.966896059721 eps: 0.1 avg reward (last 100): 910.6556959225532\n",
      "episode: 9000 episode reward: 1933.9173257095954 eps: 0.1 avg reward (last 100): 826.7034380116859\n",
      "episode: 9100 episode reward: 474.01709902216317 eps: 0.1 avg reward (last 100): 701.0929558989499\n",
      "episode: 9200 episode reward: 1252.513348288032 eps: 0.1 avg reward (last 100): 961.1616068035812\n",
      "episode: 9300 episode reward: 704.735269576473 eps: 0.1 avg reward (last 100): 1128.1672364787328\n",
      "episode: 9400 episode reward: 847.0856330565039 eps: 0.1 avg reward (last 100): 1156.6628116453091\n",
      "episode: 9500 episode reward: 2686.423616109387 eps: 0.1 avg reward (last 100): 840.7594245459752\n",
      "episode: 9600 episode reward: 3321.0513320514074 eps: 0.1 avg reward (last 100): 1628.3997736857716\n",
      "episode: 9700 episode reward: 3901.0212384286533 eps: 0.1 avg reward (last 100): 1335.2239011196402\n",
      "episode: 9800 episode reward: 1226.421064899152 eps: 0.1 avg reward (last 100): 1212.8778427664236\n",
      "episode: 9900 episode reward: 1073.0409860372529 eps: 0.1 avg reward (last 100): 1184.1667595652361\n",
      "episode: 10000 episode reward: -59.02118116674137 eps: 0.1 avg reward (last 100): 934.7794019476337\n",
      "episode: 10100 episode reward: -6391.751297747116 eps: 0.1 avg reward (last 100): 782.296733448118\n",
      "episode: 10200 episode reward: -3351.145842856802 eps: 0.1 avg reward (last 100): 1026.5495830050127\n",
      "episode: 10300 episode reward: 817.1466994600851 eps: 0.1 avg reward (last 100): 690.748691975555\n",
      "episode: 10400 episode reward: 2443.315359221697 eps: 0.1 avg reward (last 100): 974.0804993680595\n",
      "episode: 10500 episode reward: 910.7743485138908 eps: 0.1 avg reward (last 100): 608.5597928462115\n",
      "episode: 10600 episode reward: 1243.1223756058316 eps: 0.1 avg reward (last 100): 1117.2784230677053\n",
      "episode: 10700 episode reward: 5416.170656288114 eps: 0.1 avg reward (last 100): 331.90030724229456\n",
      "episode: 10800 episode reward: 1241.9917930901793 eps: 0.1 avg reward (last 100): 1469.1958432431284\n",
      "episode: 10900 episode reward: 356.9836692171466 eps: 0.1 avg reward (last 100): 461.4043283310836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-d9a5cb9b8bc6>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_games\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d9a5cb9b8bc6>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# observations is actually a single \"state\" ie past 5 days\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# TODO: REMOVE THIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprev_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\networks\\DQN.py\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, states, epsilon)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m#             return np.argmax(self.predict(np.atleast_3d(states))[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    977\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10353\u001b[0m         \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10354\u001b[0m         \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"new_axis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10355\u001b[1;33m         \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[0;32m  10356\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
