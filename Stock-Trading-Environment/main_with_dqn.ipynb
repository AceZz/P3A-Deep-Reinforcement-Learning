{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN\n",
    "from utils.utils import convert_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "N_games = 50000  # number of training games\n",
    "N_save = 500    # interval between save model, must be over 200\n",
    "model_name = \"DQN\"\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = convert_action(action)                # TODO: REMOVE THIS\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "#         rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# def make_video(env, TrainNet):\n",
    "#     env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "#     rewards = 0\n",
    "#     steps = 0\n",
    "#     done = False\n",
    "#     observation = env.reset()\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = TrainNet.get_action(observation, 0)\n",
    "#         action = convert_action(action)                # TODO: REMOVE THIS\n",
    "#         observation, reward, done, _ = env.step(action)\n",
    "#         steps += 1\n",
    "#         rewards += reward\n",
    "#     print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('./data/AAPL.csv')\n",
    "    df = df.sort_values('Date')\n",
    "    env = StockTradingEnv(df)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 3                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N_games = 50000\n",
    "    total_rewards = np.empty(N_games)\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N_games):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "        \n",
    "        # Save the model\n",
    "        if n % N_save == 0 and n>=N_save:\n",
    "            TrainNet.model.save_weights('save_models/{}_{}'.format(model_name, n), save_format='tf')\n",
    "\n",
    "        \n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the difference of net_worth between the beginning and the end of the step\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n",
<<<<<<< HEAD
<<<<<<< Updated upstream
      "episode: 0 episode reward: 2813.4527644554473 eps: 0.8991 avg reward (last 100): 2813.4527644554473\n",
      "episode: 100 episode reward: 279.39175076859465 eps: 0.8134986194699355 avg reward (last 100): 121.96589311118454\n",
      "episode: 200 episode reward: 1964.8893389911063 eps: 0.7360471625842407 avg reward (last 100): 590.8715666568311\n",
      "episode: 300 episode reward: 419.1370444178556 eps: 0.6659696926115485 avg reward (last 100): 448.7729660054564\n",
      "episode: 400 episode reward: 2329.236702562979 eps: 0.6025641480906593 avg reward (last 100): 899.2924588026743\n",
      "episode: 500 episode reward: 849.2114416451805 eps: 0.545195309324691 avg reward (last 100): 539.9068127462876\n",
      "episode: 600 episode reward: 484.07331987522593 eps: 0.49328843452021 avg reward (last 100): 761.4236627514086\n",
      "episode: 700 episode reward: 3497.1657938156113 eps: 0.44632350181590114 avg reward (last 100): 1057.5417690293111\n",
      "episode: 800 episode reward: -1107.6033283439701 eps: 0.4038299995153185 avg reward (last 100): 821.6856949960445\n",
      "episode: 900 episode reward: -267.6575216807141 eps: 0.3653822123303929 avg reward (last 100): 432.40622541160855\n",
      "episode: 1000 episode reward: 754.5534846330083 eps: 0.33059495641157327 avg reward (last 100): 613.0169621802544\n",
      "episode: 1100 episode reward: 1512.9855649156743 eps: 0.29911972043659035 avg reward (last 100): 1017.9346905252735\n",
      "episode: 1200 episode reward: 2974.3732381848986 eps: 0.27064117409787486 avg reward (last 100): 833.97599848724\n",
      "episode: 1300 episode reward: 1999.4172867224715 eps: 0.24487400900939155 avg reward (last 100): 986.4930248971857\n",
      "episode: 1400 episode reward: 727.3192918361347 eps: 0.22156008038394912 avg reward (last 100): 800.4363160715238\n",
      "episode: 1500 episode reward: -764.6301166067751 eps: 0.2004658208452793 avg reward (last 100): 851.6794601129903\n",
      "episode: 1600 episode reward: 1578.1700414024617 eps: 0.1813799004655124 avg reward (last 100): 856.9597246027346\n",
      "episode: 1700 episode reward: 1461.136870748558 eps: 0.16411110958546163 avg reward (last 100): 900.3210825620375\n",
      "episode: 1800 episode reward: 2354.429379863892 eps: 0.14848644320704313 avg reward (last 100): 923.9267010633553\n",
      "episode: 1900 episode reward: 6464.108866037306 eps: 0.13434936776657827 avg reward (last 100): 1184.2510721903116\n",
      "episode: 2000 episode reward: -4183.124126728902 eps: 0.12155825292489168 avg reward (last 100): 507.4428421069452\n",
      "episode: 2100 episode reward: 1158.8348881478396 eps: 0.109984952663304 avg reward (last 100): 1003.9865529785997\n",
      "episode: 2200 episode reward: -3589.573734753163 eps: 0.1 avg reward (last 100): 1286.380622035743\n",
      "episode: 2300 episode reward: -2184.668375245139 eps: 0.1 avg reward (last 100): 881.5508078124734\n",
      "episode: 2400 episode reward: 2672.119930256469 eps: 0.1 avg reward (last 100): 1191.7189945472285\n",
      "episode: 2500 episode reward: 982.9777901761881 eps: 0.1 avg reward (last 100): 1093.7499669073445\n",
      "episode: 2600 episode reward: -1049.8699221657498 eps: 0.1 avg reward (last 100): 1319.352126713926\n",
      "episode: 2700 episode reward: -449.2860431897716 eps: 0.1 avg reward (last 100): 685.2212074097854\n",
      "episode: 2800 episode reward: -224.26637129985102 eps: 0.1 avg reward (last 100): 1228.8874759938258\n",
      "episode: 2900 episode reward: 836.7046350768996 eps: 0.1 avg reward (last 100): 1158.9247878904232\n",
      "episode: 3000 episode reward: -788.0341058476406 eps: 0.1 avg reward (last 100): 1384.7615107877236\n",
      "episode: 3100 episode reward: 915.4304508792338 eps: 0.1 avg reward (last 100): 931.5548616434006\n",
      "episode: 3200 episode reward: 2136.271501300169 eps: 0.1 avg reward (last 100): 1153.8520917521655\n",
      "episode: 3300 episode reward: -29.93268361510127 eps: 0.1 avg reward (last 100): 826.4251358564597\n",
      "episode: 3400 episode reward: -3966.79564306288 eps: 0.1 avg reward (last 100): 560.9193841100642\n",
      "episode: 3500 episode reward: -360.34030107097715 eps: 0.1 avg reward (last 100): 638.2639424313755\n",
      "episode: 3600 episode reward: -693.5385152429662 eps: 0.1 avg reward (last 100): 203.17134795730868\n",
      "episode: 3700 episode reward: 681.4121989872401 eps: 0.1 avg reward (last 100): 922.6337760520736\n",
      "episode: 3800 episode reward: 2125.2975659794283 eps: 0.1 avg reward (last 100): 765.1113820846907\n",
      "episode: 3900 episode reward: 1424.2052260632645 eps: 0.1 avg reward (last 100): 1000.3423932444286\n",
      "episode: 4000 episode reward: 683.9129909894891 eps: 0.1 avg reward (last 100): 1253.487761269332\n",
      "episode: 4100 episode reward: 2090.6861159793243 eps: 0.1 avg reward (last 100): 847.6106159718378\n",
      "episode: 4200 episode reward: 1926.1030186452808 eps: 0.1 avg reward (last 100): 502.599439630383\n",
      "episode: 4300 episode reward: -181.48771512207168 eps: 0.1 avg reward (last 100): 1327.1417236908433\n",
      "episode: 4400 episode reward: 3330.1754854550363 eps: 0.1 avg reward (last 100): 951.8448911829333\n",
      "episode: 4500 episode reward: -1154.6252995802424 eps: 0.1 avg reward (last 100): 450.3149622984021\n",
      "episode: 4600 episode reward: 647.6896983047955 eps: 0.1 avg reward (last 100): 631.113389802816\n",
      "episode: 4700 episode reward: 910.7939340502653 eps: 0.1 avg reward (last 100): 1522.3357687329922\n",
      "episode: 4800 episode reward: -2302.236263398535 eps: 0.1 avg reward (last 100): 916.2064784146436\n",
      "episode: 4900 episode reward: 782.6738528089827 eps: 0.1 avg reward (last 100): 454.74514915436623\n",
      "episode: 5000 episode reward: 5503.659435424088 eps: 0.1 avg reward (last 100): 786.3673088772558\n",
      "episode: 5100 episode reward: -134.23548205132283 eps: 0.1 avg reward (last 100): 1142.663097903291\n",
      "episode: 5200 episode reward: 815.9073552983718 eps: 0.1 avg reward (last 100): 1048.3399590762597\n",
      "episode: 5300 episode reward: 3220.5827042257715 eps: 0.1 avg reward (last 100): 562.3678385393341\n",
      "episode: 5400 episode reward: -2932.638281853192 eps: 0.1 avg reward (last 100): 1293.0401668131701\n",
      "episode: 5500 episode reward: 161.39531294281005 eps: 0.1 avg reward (last 100): 628.9364431091334\n",
      "episode: 5600 episode reward: 260.65457819557014 eps: 0.1 avg reward (last 100): 964.2373717851978\n",
      "episode: 5700 episode reward: 1861.3520389800797 eps: 0.1 avg reward (last 100): 979.2076773878209\n",
      "episode: 5800 episode reward: -386.3747409297357 eps: 0.1 avg reward (last 100): 1489.4471428774395\n",
      "episode: 5900 episode reward: -728.6397231522478 eps: 0.1 avg reward (last 100): 176.0439947420452\n",
      "episode: 6000 episode reward: 2607.5844551741648 eps: 0.1 avg reward (last 100): 1024.2466468721482\n",
      "episode: 6100 episode reward: 2366.6851780484885 eps: 0.1 avg reward (last 100): 1072.257364415355\n",
      "episode: 6200 episode reward: 3057.7102880660077 eps: 0.1 avg reward (last 100): 847.5485363287108\n",
      "episode: 6300 episode reward: 539.372957326148 eps: 0.1 avg reward (last 100): 1281.923918293636\n",
      "episode: 6400 episode reward: 300.20351048072916 eps: 0.1 avg reward (last 100): 770.3443232237641\n",
      "episode: 6500 episode reward: 900.1155339829165 eps: 0.1 avg reward (last 100): 995.9783257849147\n",
      "episode: 6600 episode reward: -1598.2123760534287 eps: 0.1 avg reward (last 100): 906.7337259652497\n",
      "episode: 6700 episode reward: -2377.957446576922 eps: 0.1 avg reward (last 100): 1125.6335664738322\n",
      "episode: 6800 episode reward: -338.805739363881 eps: 0.1 avg reward (last 100): 1076.3409738388175\n",
      "episode: 6900 episode reward: 1613.345754493619 eps: 0.1 avg reward (last 100): 880.7764040567266\n",
      "episode: 7000 episode reward: 568.0022827162793 eps: 0.1 avg reward (last 100): 1338.9730533687423\n",
      "episode: 7100 episode reward: 840.7494469050525 eps: 0.1 avg reward (last 100): 1034.5123509512284\n",
      "episode: 7200 episode reward: 412.0607590196105 eps: 0.1 avg reward (last 100): 939.913713026527\n",
      "episode: 7300 episode reward: 6094.876236420456 eps: 0.1 avg reward (last 100): 430.871003764297\n",
      "episode: 7400 episode reward: -2898.409247904063 eps: 0.1 avg reward (last 100): 754.5751599285398\n",
      "episode: 7500 episode reward: 4319.3694133556855 eps: 0.1 avg reward (last 100): 836.074947983294\n",
      "episode: 7600 episode reward: 3983.535588236744 eps: 0.1 avg reward (last 100): 1110.2429688341788\n",
      "episode: 7700 episode reward: -1121.8444437160852 eps: 0.1 avg reward (last 100): 865.5155146327054\n",
      "episode: 7800 episode reward: 1952.7704894142516 eps: 0.1 avg reward (last 100): 1697.987691773331\n",
      "episode: 7900 episode reward: 3756.202025770295 eps: 0.1 avg reward (last 100): 699.8875629681577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8000 episode reward: 11030.267438538867 eps: 0.1 avg reward (last 100): 537.9469444741176\n",
      "episode: 8100 episode reward: -216.1120004589029 eps: 0.1 avg reward (last 100): 1422.4088388178598\n",
      "episode: 8200 episode reward: -2664.4057586174977 eps: 0.1 avg reward (last 100): 1049.9611693725378\n",
      "episode: 8300 episode reward: 1770.6275948484763 eps: 0.1 avg reward (last 100): 1111.9869029096803\n",
      "episode: 8400 episode reward: -1396.9130426986649 eps: 0.1 avg reward (last 100): 1323.053693925063\n",
      "episode: 8500 episode reward: 4563.558645094567 eps: 0.1 avg reward (last 100): 766.2635645482775\n",
      "episode: 8600 episode reward: -8664.082215470087 eps: 0.1 avg reward (last 100): 1022.2737355374836\n",
      "episode: 8700 episode reward: 5432.11405272288 eps: 0.1 avg reward (last 100): 622.9898768573042\n",
      "episode: 8800 episode reward: 1963.1648731755395 eps: 0.1 avg reward (last 100): 1374.3225716345573\n",
      "episode: 8900 episode reward: -806.966896059721 eps: 0.1 avg reward (last 100): 910.6556959225532\n",
      "episode: 9000 episode reward: 1933.9173257095954 eps: 0.1 avg reward (last 100): 826.7034380116859\n",
      "episode: 9100 episode reward: 474.01709902216317 eps: 0.1 avg reward (last 100): 701.0929558989499\n",
      "episode: 9200 episode reward: 1252.513348288032 eps: 0.1 avg reward (last 100): 961.1616068035812\n",
      "episode: 9300 episode reward: 704.735269576473 eps: 0.1 avg reward (last 100): 1128.1672364787328\n",
      "episode: 9400 episode reward: 847.0856330565039 eps: 0.1 avg reward (last 100): 1156.6628116453091\n",
      "episode: 9500 episode reward: 2686.423616109387 eps: 0.1 avg reward (last 100): 840.7594245459752\n",
      "episode: 9600 episode reward: 3321.0513320514074 eps: 0.1 avg reward (last 100): 1628.3997736857716\n",
      "episode: 9700 episode reward: 3901.0212384286533 eps: 0.1 avg reward (last 100): 1335.2239011196402\n",
      "episode: 9800 episode reward: 1226.421064899152 eps: 0.1 avg reward (last 100): 1212.8778427664236\n",
      "episode: 9900 episode reward: 1073.0409860372529 eps: 0.1 avg reward (last 100): 1184.1667595652361\n",
      "episode: 10000 episode reward: -59.02118116674137 eps: 0.1 avg reward (last 100): 934.7794019476337\n",
      "episode: 10100 episode reward: -6391.751297747116 eps: 0.1 avg reward (last 100): 782.296733448118\n",
      "episode: 10200 episode reward: -3351.145842856802 eps: 0.1 avg reward (last 100): 1026.5495830050127\n",
      "episode: 10300 episode reward: 817.1466994600851 eps: 0.1 avg reward (last 100): 690.748691975555\n",
      "episode: 10400 episode reward: 2443.315359221697 eps: 0.1 avg reward (last 100): 974.0804993680595\n",
      "episode: 10500 episode reward: 910.7743485138908 eps: 0.1 avg reward (last 100): 608.5597928462115\n",
      "episode: 10600 episode reward: 1243.1223756058316 eps: 0.1 avg reward (last 100): 1117.2784230677053\n",
      "episode: 10700 episode reward: 5416.170656288114 eps: 0.1 avg reward (last 100): 331.90030724229456\n",
      "episode: 10800 episode reward: 1241.9917930901793 eps: 0.1 avg reward (last 100): 1469.1958432431284\n",
      "episode: 10900 episode reward: 356.9836692171466 eps: 0.1 avg reward (last 100): 461.4043283310836\n"
=======
      "1\n",
      "episode: 0 episode reward: 158.3181407549455 eps: 0.8991 avg reward (last 100): 158.3181407549455\n",
      "episode: 100 episode reward: -6599.921561361349 eps: 0.8134986194699355 avg reward (last 100): 300.13891420848506\n",
      "episode: 200 episode reward: 1589.0736907507762 eps: 0.7360471625842407 avg reward (last 100): 402.51353588269666\n",
      "episode: 300 episode reward: 230.23849472909387 eps: 0.6659696926115485 avg reward (last 100): 571.9903989760005\n",
      "episode: 400 episode reward: 2210.78221789087 eps: 0.6025641480906593 avg reward (last 100): 433.8758164833327\n",
      "episode: 500 episode reward: 2124.971890663881 eps: 0.545195309324691 avg reward (last 100): 175.2298910870765\n",
      "episode: 600 episode reward: 1310.5144479620576 eps: 0.49328843452021 avg reward (last 100): 561.8496383456464\n",
      "episode: 700 episode reward: 89.61541611480061 eps: 0.44632350181590114 avg reward (last 100): 376.6672624229842\n",
      "episode: 800 episode reward: 1883.038675370648 eps: 0.4038299995153185 avg reward (last 100): 255.4492896473041\n",
      "episode: 900 episode reward: 568.8439813006244 eps: 0.3653822123303929 avg reward (last 100): 537.7511645292683\n",
      "episode: 1000 episode reward: -1486.8887453142052 eps: 0.33059495641157327 avg reward (last 100): 453.45253689981445\n",
      "episode: 1100 episode reward: -5613.215678345735 eps: 0.29911972043659035 avg reward (last 100): 213.68910889215724\n",
      "episode: 1200 episode reward: 955.1326103433785 eps: 0.27064117409787486 avg reward (last 100): 475.7906565106855\n"
>>>>>>> Stashed changes
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
<<<<<<< Updated upstream
      "\u001b[1;32m<ipython-input-3-d9a5cb9b8bc6>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_games\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d9a5cb9b8bc6>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# observations is actually a single \"state\" ie past 5 days\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# TODO: REMOVE THIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprev_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\networks\\DQN.py\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, states, epsilon)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m#             return np.argmax(self.predict(np.atleast_3d(states))[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m    977\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\P3A_DRL\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m  10353\u001b[0m         \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10354\u001b[0m         \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"new_axis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10355\u001b[1;33m         \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[0;32m  10356\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
=======
      "\u001b[1;32m<ipython-input-3-68cd23a8c05f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_games\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-68cd23a8c05f>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         env.render()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# observations is actually a single \"state\" ie past 5 days\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# TODO: REMOVE THIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprev_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\networks\\DQN.py\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, states, epsilon)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0madd_experience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_experiences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\networks\\DQN.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# necessary for batch size of one when playing games\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
>>>>>>> Stashed changes
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
=======
      "episode: 0 episode reward: 576.4314990782987 eps: 0.8991 avg reward (last 100): 576.4314990782987\n",
      "episode: 100 episode reward: 2091.9110573285834 eps: 0.8134986194699355 avg reward (last 100): 437.81961902871564\n",
      "episode: 200 episode reward: -692.3311444692681 eps: 0.7360471625842407 avg reward (last 100): 557.6891752292472\n",
      "episode: 300 episode reward: 708.5265815975999 eps: 0.6659696926115485 avg reward (last 100): 310.81851682017873\n",
      "episode: 400 episode reward: 639.9018487561952 eps: 0.6025641480906593 avg reward (last 100): 326.7230239058313\n",
      "episode: 500 episode reward: -4.293368143782573 eps: 0.545195309324691 avg reward (last 100): 128.43589237196207\n",
      "episode: 600 episode reward: -1625.946387162512 eps: 0.49328843452021 avg reward (last 100): 99.54117269678031\n",
      "episode: 700 episode reward: 293.9183725018902 eps: 0.44632350181590114 avg reward (last 100): 172.0062252980823\n",
      "episode: 800 episode reward: 274.6325225318542 eps: 0.4038299995153185 avg reward (last 100): 169.97206282465692\n",
      "episode: 900 episode reward: 424.96496951250265 eps: 0.3653822123303929 avg reward (last 100): 214.75030468585803\n",
      "episode: 1000 episode reward: 548.0852593944546 eps: 0.33059495641157327 avg reward (last 100): 81.35768351547763\n",
      "episode: 1100 episode reward: 580.969534482474 eps: 0.29911972043659035 avg reward (last 100): 125.1364252106254\n",
      "episode: 1200 episode reward: 217.0144941658018 eps: 0.27064117409787486 avg reward (last 100): 180.49728065156992\n",
      "episode: 1300 episode reward: -115.56683302048805 eps: 0.24487400900939155 avg reward (last 100): 164.18226989804333\n",
      "episode: 1400 episode reward: -69.96685277051984 eps: 0.22156008038394912 avg reward (last 100): 54.48976184630012\n",
      "episode: 1500 episode reward: 444.83734479963096 eps: 0.2004658208452793 avg reward (last 100): 87.76024049753602\n",
      "episode: 1600 episode reward: 38.51252255068539 eps: 0.1813799004655124 avg reward (last 100): 88.05948776339714\n",
      "episode: 1700 episode reward: -23.892799364857638 eps: 0.16411110958546163 avg reward (last 100): 56.37428482165684\n",
      "episode: 1800 episode reward: 167.93295678283903 eps: 0.14848644320704313 avg reward (last 100): -20.90336187392016\n",
      "episode: 1900 episode reward: 79.80320977413612 eps: 0.13434936776657827 avg reward (last 100): 15.112931711240133\n",
      "episode: 2000 episode reward: 116.11113343842953 eps: 0.12155825292489168 avg reward (last 100): 88.16980591047322\n",
      "episode: 2100 episode reward: -212.4336512306745 eps: 0.109984952663304 avg reward (last 100): 120.50655285934252\n",
      "episode: 2200 episode reward: -38.082545096500326 eps: 0.1 avg reward (last 100): 35.45839730947481\n",
      "episode: 2300 episode reward: 708.2938935525235 eps: 0.1 avg reward (last 100): 60.868835485257755\n",
      "episode: 2400 episode reward: 104.07992396059126 eps: 0.1 avg reward (last 100): 52.207845551885065\n",
      "episode: 2500 episode reward: 366.68037797098805 eps: 0.1 avg reward (last 100): 68.51981858114029\n",
      "episode: 2600 episode reward: -873.3510619837925 eps: 0.1 avg reward (last 100): 50.30180653025743\n",
      "episode: 2700 episode reward: 0.0 eps: 0.1 avg reward (last 100): 54.40296361142345\n",
      "episode: 2800 episode reward: 195.39215949839672 eps: 0.1 avg reward (last 100): 21.519390665251464\n",
      "episode: 2900 episode reward: 883.4167361249729 eps: 0.1 avg reward (last 100): 30.367943094181264\n",
      "episode: 3000 episode reward: 356.794208715708 eps: 0.1 avg reward (last 100): 53.81637733464934\n",
      "episode: 3100 episode reward: -273.8769152148725 eps: 0.1 avg reward (last 100): 38.30730573217124\n",
      "episode: 3200 episode reward: -67.21949017584484 eps: 0.1 avg reward (last 100): 54.201646761997225\n",
      "episode: 3300 episode reward: -154.19999997153718 eps: 0.1 avg reward (last 100): 21.620753869210397\n",
      "episode: 3400 episode reward: -79.70800731907366 eps: 0.1 avg reward (last 100): 40.29788391734394\n",
      "episode: 3500 episode reward: 31.849470551202103 eps: 0.1 avg reward (last 100): 82.68166911057352\n",
      "episode: 3600 episode reward: -50.9481052557403 eps: 0.1 avg reward (last 100): 26.83989925013295\n",
      "episode: 3700 episode reward: -240.72454177953114 eps: 0.1 avg reward (last 100): 26.542886579401046\n",
      "episode: 3800 episode reward: 217.6556989680339 eps: 0.1 avg reward (last 100): 7.1898002009468716\n",
      "episode: 3900 episode reward: 85.6183574508268 eps: 0.1 avg reward (last 100): 52.143329126246165\n",
      "episode: 4000 episode reward: 226.4349726915334 eps: 0.1 avg reward (last 100): 40.03438804705635\n",
      "episode: 4100 episode reward: -824.3003842347462 eps: 0.1 avg reward (last 100): -9.697975706693743\n",
      "episode: 4200 episode reward: -118.39118796037656 eps: 0.1 avg reward (last 100): -1.7685260448220872\n",
      "episode: 4300 episode reward: 23.830564946096274 eps: 0.1 avg reward (last 100): 33.21531216338923\n",
      "episode: 4400 episode reward: 179.07327721772344 eps: 0.1 avg reward (last 100): 74.31443805651219\n",
      "episode: 4500 episode reward: -44.235483302243665 eps: 0.1 avg reward (last 100): 37.0283825101229\n",
      "episode: 4600 episode reward: -33.209986838903205 eps: 0.1 avg reward (last 100): 23.099688592595204\n",
      "episode: 4700 episode reward: 123.91538625495559 eps: 0.1 avg reward (last 100): -3.703920362181067\n",
      "episode: 4800 episode reward: 106.91013001411375 eps: 0.1 avg reward (last 100): 104.40503807264805\n",
      "episode: 4900 episode reward: 119.96254810641949 eps: 0.1 avg reward (last 100): 48.041479835158455\n",
      "episode: 5000 episode reward: 63.79557865221432 eps: 0.1 avg reward (last 100): 94.79489626397735\n",
      "episode: 5100 episode reward: 19.82499979131535 eps: 0.1 avg reward (last 100): 56.765754768417715\n",
      "episode: 5200 episode reward: 75.55842731945268 eps: 0.1 avg reward (last 100): 36.41230361442649\n",
      "episode: 5300 episode reward: 188.41408520219375 eps: 0.1 avg reward (last 100): 45.37751036829111\n",
      "episode: 5400 episode reward: 77.91970733945891 eps: 0.1 avg reward (last 100): 3.5287547379488173\n",
      "episode: 5500 episode reward: -14.007154995662859 eps: 0.1 avg reward (last 100): 65.08265792452828\n",
      "episode: 5600 episode reward: 148.23405775558786 eps: 0.1 avg reward (last 100): 73.52369715004397\n",
      "episode: 5700 episode reward: -79.44318172254862 eps: 0.1 avg reward (last 100): 81.25475981289827\n",
      "episode: 5800 episode reward: -48.57191356327894 eps: 0.1 avg reward (last 100): 38.029251908244305\n",
      "episode: 5900 episode reward: 72.08111016321709 eps: 0.1 avg reward (last 100): 82.23536912508536\n",
      "episode: 6000 episode reward: -320.5907846466289 eps: 0.1 avg reward (last 100): -6.8373960789595545\n",
      "episode: 6100 episode reward: -55.83973236170277 eps: 0.1 avg reward (last 100): 59.383169381309024\n",
      "episode: 6200 episode reward: 58.48584811465844 eps: 0.1 avg reward (last 100): 5.072197228939691\n",
      "episode: 6300 episode reward: -55.9745650670211 eps: 0.1 avg reward (last 100): 66.831787528443\n",
      "episode: 6400 episode reward: 285.72066689535677 eps: 0.1 avg reward (last 100): 13.235291311862209\n",
      "episode: 6500 episode reward: -54.40645984033108 eps: 0.1 avg reward (last 100): 43.554794804875975\n",
      "episode: 6600 episode reward: -53.19191528761803 eps: 0.1 avg reward (last 100): 49.58446059734644\n",
      "episode: 6700 episode reward: 352.97447417985677 eps: 0.1 avg reward (last 100): 41.17711259496261\n",
      "episode: 6800 episode reward: 66.29646544325624 eps: 0.1 avg reward (last 100): 23.658768945785567\n",
      "episode: 6900 episode reward: 73.63825390975035 eps: 0.1 avg reward (last 100): 12.213427244776666\n",
      "episode: 7000 episode reward: 2.1206555838616623 eps: 0.1 avg reward (last 100): 49.42404032861214\n",
      "episode: 7100 episode reward: 263.5257774369875 eps: 0.1 avg reward (last 100): 43.88852365878436\n",
      "episode: 7200 episode reward: 471.9616149430676 eps: 0.1 avg reward (last 100): 90.06201945544649\n",
      "episode: 7300 episode reward: 71.20201348875707 eps: 0.1 avg reward (last 100): 59.138372895245276\n",
      "episode: 7400 episode reward: -111.26241173735434 eps: 0.1 avg reward (last 100): 19.701929455766766\n",
      "episode: 7500 episode reward: 153.68604129368214 eps: 0.1 avg reward (last 100): 95.3839087872783\n",
      "episode: 7600 episode reward: -250.2522737901836 eps: 0.1 avg reward (last 100): 64.02658944612207\n",
      "episode: 7700 episode reward: 240.0722393921642 eps: 0.1 avg reward (last 100): 27.890885435819754\n",
      "episode: 7800 episode reward: -471.6235241565282 eps: 0.1 avg reward (last 100): 50.04590847106869\n",
      "episode: 7900 episode reward: -12.255877571267774 eps: 0.1 avg reward (last 100): 48.16124157083027\n",
      "episode: 8000 episode reward: -200.47849683845743 eps: 0.1 avg reward (last 100): 18.479881497502397\n",
      "episode: 8100 episode reward: -142.43789780977204 eps: 0.1 avg reward (last 100): -21.628733920393948\n",
      "episode: 8200 episode reward: 77.46843807205187 eps: 0.1 avg reward (last 100): 50.6183559877763\n",
      "episode: 8300 episode reward: -85.32405437255329 eps: 0.1 avg reward (last 100): 57.92783873523725\n",
      "episode: 8400 episode reward: -10.176353003118493 eps: 0.1 avg reward (last 100): 39.51856782473204\n",
      "episode: 8500 episode reward: -44.25957923550595 eps: 0.1 avg reward (last 100): 27.732635056452125\n",
      "episode: 8600 episode reward: 606.0504278943026 eps: 0.1 avg reward (last 100): 69.95117803943975\n",
      "episode: 8700 episode reward: -27.61449779304712 eps: 0.1 avg reward (last 100): 68.70479398245637\n",
      "episode: 8800 episode reward: 223.6402420833001 eps: 0.1 avg reward (last 100): 88.42711026475601\n",
      "episode: 8900 episode reward: 0.0 eps: 0.1 avg reward (last 100): 51.37131124199692\n",
      "episode: 9000 episode reward: 39.639093883793976 eps: 0.1 avg reward (last 100): 27.472639743770248\n",
      "episode: 9100 episode reward: -10.991365081379627 eps: 0.1 avg reward (last 100): 31.830105554668044\n",
      "episode: 9200 episode reward: -137.27791378325128 eps: 0.1 avg reward (last 100): 60.263872091306666\n",
      "episode: 9300 episode reward: -73.42457955099417 eps: 0.1 avg reward (last 100): 121.16122673076701\n",
      "episode: 9400 episode reward: 123.58763668987922 eps: 0.1 avg reward (last 100): 57.58106280216118\n",
      "episode: 9500 episode reward: 22.791201562364222 eps: 0.1 avg reward (last 100): 41.79151450844586\n"
     ],
     "output_type": "stream"
>>>>>>> master
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (reinforcement-learning-master)",
   "language": "python",
   "name": "pycharm-7a748578"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.7.6"
=======
   "version": "3.7.5"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}