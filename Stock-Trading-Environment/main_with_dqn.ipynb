{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_action(action):\n",
    "    \"\"\"TEMPORARY FUNCTION, NEED TO GO TO CONTINUOUS ACTIONS\"\"\"\n",
    "    if action == 0:\n",
    "        return np.array([0, 0.2]) #buy stocks with 20% of remaining balance\n",
    "    elif action == 1:\n",
    "        return np.array([1, 0.2]) #sell 20% of stocks\n",
    "    elif action == 2:\n",
    "        return np.array([2, 0]) #do nothing\n",
    "\n",
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        action = convert_action(action)                # TODO: REMOVE THIS\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "#         rewards += reward\n",
    "        rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# def make_video(env, TrainNet):\n",
    "#     env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "#     rewards = 0\n",
    "#     steps = 0\n",
    "#     done = False\n",
    "#     observation = env.reset()\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = TrainNet.get_action(observation, 0)\n",
    "#         action = convert_action(action)                # TODO: REMOVE THIS\n",
    "#         observation, reward, done, _ = env.step(action)\n",
    "#         steps += 1\n",
    "#         rewards += reward\n",
    "#     print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('./data/AAPL.csv')\n",
    "    df = df.sort_values('Date')\n",
    "    env = StockTradingEnv(df)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 3                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    batch_size = 32\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N = 50000\n",
    "    total_rewards = np.empty(N)\n",
    "    epsilon = 0.5\n",
    "    decay = 0.99\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the account balance at the end of the episode (to be modified?)\n",
    "<br>\n",
    "Initial account balance is 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n",
      "episode: 0 episode reward: 7540.249322142716 eps: 0.495 avg reward (last 100): 7540.249322142716\n",
      "episode: 50 episode reward: 9383.202027016032 eps: 0.29947800323308055 avg reward (last 100): 8239.970025648254\n",
      "episode: 100 episode reward: 8854.51834679909 eps: 0.18118600893024847 avg reward (last 100): 8480.816789366045\n",
      "episode: 150 episode reward: 9038.616959685365 eps: 0.10961863468323617 avg reward (last 100): 8745.677252869926\n",
      "episode: 200 episode reward: 9917.25632793308 eps: 0.1 avg reward (last 100): 9025.53782366849\n",
      "episode: 250 episode reward: 9477.824648810472 eps: 0.1 avg reward (last 100): 9318.948323564175\n",
      "episode: 300 episode reward: 7517.225602372908 eps: 0.1 avg reward (last 100): 9244.209838669503\n",
      "episode: 350 episode reward: 9660.034702017394 eps: 0.1 avg reward (last 100): 9163.528873234936\n",
      "episode: 400 episode reward: 8629.020854970011 eps: 0.1 avg reward (last 100): 9081.680543794933\n",
      "episode: 450 episode reward: 10092.410226076041 eps: 0.1 avg reward (last 100): 9079.392031221683\n",
      "episode: 500 episode reward: 9597.621503493008 eps: 0.1 avg reward (last 100): 9107.85403082505\n",
      "episode: 550 episode reward: 9462.087640559912 eps: 0.1 avg reward (last 100): 9194.599205037608\n",
      "episode: 600 episode reward: 9776.689148575357 eps: 0.1 avg reward (last 100): 9243.033328090129\n",
      "episode: 650 episode reward: 9642.23806626834 eps: 0.1 avg reward (last 100): 9161.661032181768\n",
      "episode: 700 episode reward: 9469.610283220896 eps: 0.1 avg reward (last 100): 9254.833218212441\n",
      "episode: 750 episode reward: 9948.72321844837 eps: 0.1 avg reward (last 100): 9298.610435090277\n",
      "episode: 800 episode reward: 7621.196522698612 eps: 0.1 avg reward (last 100): 9179.147303789247\n",
      "episode: 850 episode reward: 7643.720904786067 eps: 0.1 avg reward (last 100): 9129.736987271506\n",
      "episode: 900 episode reward: 9183.04528694727 eps: 0.1 avg reward (last 100): 9173.222426950924\n",
      "episode: 950 episode reward: 9788.675989646745 eps: 0.1 avg reward (last 100): 9117.315647215291\n",
      "episode: 1000 episode reward: 9912.070124584608 eps: 0.1 avg reward (last 100): 9161.443402417168\n",
      "episode: 1050 episode reward: 9945.787300456965 eps: 0.1 avg reward (last 100): 9232.161178963648\n",
      "episode: 1100 episode reward: 9677.997016847237 eps: 0.1 avg reward (last 100): 9142.693098087115\n",
      "episode: 1150 episode reward: 10028.076566935 eps: 0.1 avg reward (last 100): 9201.771887919453\n",
      "episode: 1200 episode reward: 9938.861054328823 eps: 0.1 avg reward (last 100): 9183.410020184076\n",
      "episode: 1250 episode reward: 10085.285577315393 eps: 0.1 avg reward (last 100): 9123.99060612339\n",
      "episode: 1300 episode reward: 10086.293216351603 eps: 0.1 avg reward (last 100): 9178.286011361453\n",
      "episode: 1350 episode reward: 8049.426157023575 eps: 0.1 avg reward (last 100): 9230.873350259264\n",
      "episode: 1400 episode reward: 8314.827787449327 eps: 0.1 avg reward (last 100): 9112.366232275292\n",
      "episode: 1450 episode reward: 9835.84675909038 eps: 0.1 avg reward (last 100): 9129.585207997497\n",
      "episode: 1500 episode reward: 9583.594216109668 eps: 0.1 avg reward (last 100): 9240.562887064629\n",
      "episode: 1550 episode reward: 9306.089729632375 eps: 0.1 avg reward (last 100): 9233.607527553471\n",
      "episode: 1600 episode reward: 10000.0 eps: 0.1 avg reward (last 100): 9236.125600683208\n",
      "episode: 1650 episode reward: 8947.702607005494 eps: 0.1 avg reward (last 100): 9176.03448913132\n",
      "episode: 1700 episode reward: 8906.776550411196 eps: 0.1 avg reward (last 100): 9167.350365578252\n",
      "episode: 1750 episode reward: 10000.0 eps: 0.1 avg reward (last 100): 9201.177444226858\n",
      "episode: 1800 episode reward: 10055.493915242125 eps: 0.1 avg reward (last 100): 9221.186512110644\n",
      "episode: 1850 episode reward: 9763.718598320962 eps: 0.1 avg reward (last 100): 9213.218468792138\n",
      "episode: 1900 episode reward: 9498.552109781753 eps: 0.1 avg reward (last 100): 9175.514445241251\n",
      "episode: 1950 episode reward: 9845.686366623811 eps: 0.1 avg reward (last 100): 9159.724213095697\n",
      "episode: 2000 episode reward: 9574.969119960308 eps: 0.1 avg reward (last 100): 9284.618373037003\n",
      "episode: 2050 episode reward: 10202.513430108385 eps: 0.1 avg reward (last 100): 9292.664377535953\n",
      "episode: 2100 episode reward: 10048.632232562806 eps: 0.1 avg reward (last 100): 9246.217012525858\n",
      "episode: 2150 episode reward: 9832.257422452494 eps: 0.1 avg reward (last 100): 9209.373543185548\n",
      "episode: 2200 episode reward: 9125.600882257557 eps: 0.1 avg reward (last 100): 9241.100419980074\n",
      "episode: 2250 episode reward: 9068.303223061095 eps: 0.1 avg reward (last 100): 9346.259555332446\n",
      "episode: 2300 episode reward: 7319.514289568605 eps: 0.1 avg reward (last 100): 9224.935706125289\n",
      "episode: 2350 episode reward: 9420.259804475601 eps: 0.1 avg reward (last 100): 9208.591164324258\n",
      "episode: 2400 episode reward: 8104.78477958185 eps: 0.1 avg reward (last 100): 9089.413035526082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-d2addea4a9d6>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-d2addea4a9d6>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;31m# TODO: REMOVE THIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprev_observations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m#         rewards += reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_worth\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_observation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Polytechnique\\P3A_DRL\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\env\\StockTradingEnv.py\u001b[0m in \u001b[0;36m_next_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m                         5, 'Close'].values / MAX_SHARE_PRICE,\n\u001b[0;32m     46\u001b[0m             self.df.loc[self.current_step: self.current_step +\n\u001b[1;32m---> 47\u001b[1;33m                         5, 'Volume'].values / MAX_NUM_SHARES,\n\u001b[0m\u001b[0;32m     48\u001b[0m         ])\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/AAPL.csv')\n",
    "df = df.sort_values('Date')\n",
    "env = StockTradingEnv(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
