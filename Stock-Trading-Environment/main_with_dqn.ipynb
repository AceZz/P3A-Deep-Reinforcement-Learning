{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQN import DQN\n",
    "from utils.utils import convert_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from medium article (as well for networks/DQN.py): https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "N_games = 50000  # number of training games\n",
    "N_save = 500    # interval between save model, must be over 200\n",
    "model_name = \"DQN\"\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    steps = 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = TrainNet.get_action(observations, epsilon) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = convert_action(action)                # TODO: REMOVE THIS\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "#         rewards = reward\n",
    "        if done:\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        if steps >= 100: # Limiting the number of steps\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        TrainNet.train(TargetNet)\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "        steps += 1\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# def make_video(env, TrainNet):\n",
    "#     env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "#     rewards = 0\n",
    "#     steps = 0\n",
    "#     done = False\n",
    "#     observation = env.reset()\n",
    "#     while not done:\n",
    "#         env.render()\n",
    "#         action = TrainNet.get_action(observation, 0)\n",
    "#         action = convert_action(action)                # TODO: REMOVE THIS\n",
    "#         observation, reward, done, _ = env.step(action)\n",
    "#         steps += 1\n",
    "#         rewards += reward\n",
    "#     print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('./data/AAPL.csv')\n",
    "    df = df.sort_values('Date')\n",
    "    env = StockTradingEnv(df)\n",
    "    gamma = 0.99\n",
    "    copy_step = 25\n",
    "#     num_states = len(env.observation_space.sample())\n",
    "    input_shape = env.observation_space.sample().shape\n",
    "#     num_actions = env.action_space.n\n",
    "    num_actions = 3                        # TODO: CHANGE THIS TO CONTINUOUS VALUES\n",
    "    hidden_units = [200, 200]\n",
    "    max_experiences = 10000\n",
    "    min_experiences = 100\n",
    "    lr = 1e-2\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = 'logs/dqn/' + current_time\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    TrainNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    TargetNet = DQN(input_shape, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    print(\"DeepQ Networks successfully created\")\n",
    "    N_games = 50000\n",
    "    total_rewards = np.empty(N_games)\n",
    "    epsilon = 0.9\n",
    "    decay = 0.999\n",
    "    min_epsilon = 0.1\n",
    "    print(\"Starting training...\")\n",
    "    for n in range(N_games):\n",
    "        epsilon = max(min_epsilon, epsilon * decay)\n",
    "        total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
    "        total_rewards[n] = total_reward\n",
    "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards)\n",
    "        \n",
    "        # Save the model\n",
    "        if n % N_save == 0 and n>=N_save:\n",
    "            TrainNet.model.save_weights('save_models/{}_{}'.format(model_name, n), save_format='tf')\n",
    "\n",
    "        \n",
    "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
    "#     make_video(env, TrainNet)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The episode reward is the difference of net_worth between the beginning and the end of the step\n",
    "<br>\n",
    "Initial account balance is 10,000\n",
    "<br>\n",
    "To see live results in Tensorboard: tensorboard --logdir *log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "DeepQ Networks successfully created\n",
      "Starting training...\n",
      "episode: 0 episode reward: 576.4314990782987 eps: 0.8991 avg reward (last 100): 576.4314990782987\n",
      "episode: 100 episode reward: 2091.9110573285834 eps: 0.8134986194699355 avg reward (last 100): 437.81961902871564\n",
      "episode: 200 episode reward: -692.3311444692681 eps: 0.7360471625842407 avg reward (last 100): 557.6891752292472\n",
      "episode: 300 episode reward: 708.5265815975999 eps: 0.6659696926115485 avg reward (last 100): 310.81851682017873\n",
      "episode: 400 episode reward: 639.9018487561952 eps: 0.6025641480906593 avg reward (last 100): 326.7230239058313\n",
      "episode: 500 episode reward: -4.293368143782573 eps: 0.545195309324691 avg reward (last 100): 128.43589237196207\n",
      "episode: 600 episode reward: -1625.946387162512 eps: 0.49328843452021 avg reward (last 100): 99.54117269678031\n",
      "episode: 700 episode reward: 293.9183725018902 eps: 0.44632350181590114 avg reward (last 100): 172.0062252980823\n",
      "episode: 800 episode reward: 274.6325225318542 eps: 0.4038299995153185 avg reward (last 100): 169.97206282465692\n",
      "episode: 900 episode reward: 424.96496951250265 eps: 0.3653822123303929 avg reward (last 100): 214.75030468585803\n",
      "episode: 1000 episode reward: 548.0852593944546 eps: 0.33059495641157327 avg reward (last 100): 81.35768351547763\n",
      "episode: 1100 episode reward: 580.969534482474 eps: 0.29911972043659035 avg reward (last 100): 125.1364252106254\n",
      "episode: 1200 episode reward: 217.0144941658018 eps: 0.27064117409787486 avg reward (last 100): 180.49728065156992\n",
      "episode: 1300 episode reward: -115.56683302048805 eps: 0.24487400900939155 avg reward (last 100): 164.18226989804333\n",
      "episode: 1400 episode reward: -69.96685277051984 eps: 0.22156008038394912 avg reward (last 100): 54.48976184630012\n",
      "episode: 1500 episode reward: 444.83734479963096 eps: 0.2004658208452793 avg reward (last 100): 87.76024049753602\n",
      "episode: 1600 episode reward: 38.51252255068539 eps: 0.1813799004655124 avg reward (last 100): 88.05948776339714\n",
      "episode: 1700 episode reward: -23.892799364857638 eps: 0.16411110958546163 avg reward (last 100): 56.37428482165684\n",
      "episode: 1800 episode reward: 167.93295678283903 eps: 0.14848644320704313 avg reward (last 100): -20.90336187392016\n",
      "episode: 1900 episode reward: 79.80320977413612 eps: 0.13434936776657827 avg reward (last 100): 15.112931711240133\n",
      "episode: 2000 episode reward: 116.11113343842953 eps: 0.12155825292489168 avg reward (last 100): 88.16980591047322\n",
      "episode: 2100 episode reward: -212.4336512306745 eps: 0.109984952663304 avg reward (last 100): 120.50655285934252\n",
      "episode: 2200 episode reward: -38.082545096500326 eps: 0.1 avg reward (last 100): 35.45839730947481\n",
      "episode: 2300 episode reward: 708.2938935525235 eps: 0.1 avg reward (last 100): 60.868835485257755\n",
      "episode: 2400 episode reward: 104.07992396059126 eps: 0.1 avg reward (last 100): 52.207845551885065\n",
      "episode: 2500 episode reward: 366.68037797098805 eps: 0.1 avg reward (last 100): 68.51981858114029\n",
      "episode: 2600 episode reward: -873.3510619837925 eps: 0.1 avg reward (last 100): 50.30180653025743\n",
      "episode: 2700 episode reward: 0.0 eps: 0.1 avg reward (last 100): 54.40296361142345\n",
      "episode: 2800 episode reward: 195.39215949839672 eps: 0.1 avg reward (last 100): 21.519390665251464\n",
      "episode: 2900 episode reward: 883.4167361249729 eps: 0.1 avg reward (last 100): 30.367943094181264\n",
      "episode: 3000 episode reward: 356.794208715708 eps: 0.1 avg reward (last 100): 53.81637733464934\n",
      "episode: 3100 episode reward: -273.8769152148725 eps: 0.1 avg reward (last 100): 38.30730573217124\n",
      "episode: 3200 episode reward: -67.21949017584484 eps: 0.1 avg reward (last 100): 54.201646761997225\n",
      "episode: 3300 episode reward: -154.19999997153718 eps: 0.1 avg reward (last 100): 21.620753869210397\n",
      "episode: 3400 episode reward: -79.70800731907366 eps: 0.1 avg reward (last 100): 40.29788391734394\n",
      "episode: 3500 episode reward: 31.849470551202103 eps: 0.1 avg reward (last 100): 82.68166911057352\n",
      "episode: 3600 episode reward: -50.9481052557403 eps: 0.1 avg reward (last 100): 26.83989925013295\n",
      "episode: 3700 episode reward: -240.72454177953114 eps: 0.1 avg reward (last 100): 26.542886579401046\n",
      "episode: 3800 episode reward: 217.6556989680339 eps: 0.1 avg reward (last 100): 7.1898002009468716\n",
      "episode: 3900 episode reward: 85.6183574508268 eps: 0.1 avg reward (last 100): 52.143329126246165\n",
      "episode: 4000 episode reward: 226.4349726915334 eps: 0.1 avg reward (last 100): 40.03438804705635\n",
      "episode: 4100 episode reward: -824.3003842347462 eps: 0.1 avg reward (last 100): -9.697975706693743\n",
      "episode: 4200 episode reward: -118.39118796037656 eps: 0.1 avg reward (last 100): -1.7685260448220872\n",
      "episode: 4300 episode reward: 23.830564946096274 eps: 0.1 avg reward (last 100): 33.21531216338923\n",
      "episode: 4400 episode reward: 179.07327721772344 eps: 0.1 avg reward (last 100): 74.31443805651219\n",
      "episode: 4500 episode reward: -44.235483302243665 eps: 0.1 avg reward (last 100): 37.0283825101229\n",
      "episode: 4600 episode reward: -33.209986838903205 eps: 0.1 avg reward (last 100): 23.099688592595204\n",
      "episode: 4700 episode reward: 123.91538625495559 eps: 0.1 avg reward (last 100): -3.703920362181067\n",
      "episode: 4800 episode reward: 106.91013001411375 eps: 0.1 avg reward (last 100): 104.40503807264805\n",
      "episode: 4900 episode reward: 119.96254810641949 eps: 0.1 avg reward (last 100): 48.041479835158455\n",
      "episode: 5000 episode reward: 63.79557865221432 eps: 0.1 avg reward (last 100): 94.79489626397735\n",
      "episode: 5100 episode reward: 19.82499979131535 eps: 0.1 avg reward (last 100): 56.765754768417715\n",
      "episode: 5200 episode reward: 75.55842731945268 eps: 0.1 avg reward (last 100): 36.41230361442649\n",
      "episode: 5300 episode reward: 188.41408520219375 eps: 0.1 avg reward (last 100): 45.37751036829111\n",
      "episode: 5400 episode reward: 77.91970733945891 eps: 0.1 avg reward (last 100): 3.5287547379488173\n",
      "episode: 5500 episode reward: -14.007154995662859 eps: 0.1 avg reward (last 100): 65.08265792452828\n",
      "episode: 5600 episode reward: 148.23405775558786 eps: 0.1 avg reward (last 100): 73.52369715004397\n",
      "episode: 5700 episode reward: -79.44318172254862 eps: 0.1 avg reward (last 100): 81.25475981289827\n",
      "episode: 5800 episode reward: -48.57191356327894 eps: 0.1 avg reward (last 100): 38.029251908244305\n",
      "episode: 5900 episode reward: 72.08111016321709 eps: 0.1 avg reward (last 100): 82.23536912508536\n",
      "episode: 6000 episode reward: -320.5907846466289 eps: 0.1 avg reward (last 100): -6.8373960789595545\n",
      "episode: 6100 episode reward: -55.83973236170277 eps: 0.1 avg reward (last 100): 59.383169381309024\n",
      "episode: 6200 episode reward: 58.48584811465844 eps: 0.1 avg reward (last 100): 5.072197228939691\n",
      "episode: 6300 episode reward: -55.9745650670211 eps: 0.1 avg reward (last 100): 66.831787528443\n",
      "episode: 6400 episode reward: 285.72066689535677 eps: 0.1 avg reward (last 100): 13.235291311862209\n",
      "episode: 6500 episode reward: -54.40645984033108 eps: 0.1 avg reward (last 100): 43.554794804875975\n",
      "episode: 6600 episode reward: -53.19191528761803 eps: 0.1 avg reward (last 100): 49.58446059734644\n",
      "episode: 6700 episode reward: 352.97447417985677 eps: 0.1 avg reward (last 100): 41.17711259496261\n",
      "episode: 6800 episode reward: 66.29646544325624 eps: 0.1 avg reward (last 100): 23.658768945785567\n",
      "episode: 6900 episode reward: 73.63825390975035 eps: 0.1 avg reward (last 100): 12.213427244776666\n",
      "episode: 7000 episode reward: 2.1206555838616623 eps: 0.1 avg reward (last 100): 49.42404032861214\n",
      "episode: 7100 episode reward: 263.5257774369875 eps: 0.1 avg reward (last 100): 43.88852365878436\n",
      "episode: 7200 episode reward: 471.9616149430676 eps: 0.1 avg reward (last 100): 90.06201945544649\n",
      "episode: 7300 episode reward: 71.20201348875707 eps: 0.1 avg reward (last 100): 59.138372895245276\n",
      "episode: 7400 episode reward: -111.26241173735434 eps: 0.1 avg reward (last 100): 19.701929455766766\n",
      "episode: 7500 episode reward: 153.68604129368214 eps: 0.1 avg reward (last 100): 95.3839087872783\n",
      "episode: 7600 episode reward: -250.2522737901836 eps: 0.1 avg reward (last 100): 64.02658944612207\n",
      "episode: 7700 episode reward: 240.0722393921642 eps: 0.1 avg reward (last 100): 27.890885435819754\n",
      "episode: 7800 episode reward: -471.6235241565282 eps: 0.1 avg reward (last 100): 50.04590847106869\n",
      "episode: 7900 episode reward: -12.255877571267774 eps: 0.1 avg reward (last 100): 48.16124157083027\n",
      "episode: 8000 episode reward: -200.47849683845743 eps: 0.1 avg reward (last 100): 18.479881497502397\n",
      "episode: 8100 episode reward: -142.43789780977204 eps: 0.1 avg reward (last 100): -21.628733920393948\n",
      "episode: 8200 episode reward: 77.46843807205187 eps: 0.1 avg reward (last 100): 50.6183559877763\n",
      "episode: 8300 episode reward: -85.32405437255329 eps: 0.1 avg reward (last 100): 57.92783873523725\n",
      "episode: 8400 episode reward: -10.176353003118493 eps: 0.1 avg reward (last 100): 39.51856782473204\n",
      "episode: 8500 episode reward: -44.25957923550595 eps: 0.1 avg reward (last 100): 27.732635056452125\n",
      "episode: 8600 episode reward: 606.0504278943026 eps: 0.1 avg reward (last 100): 69.95117803943975\n",
      "episode: 8700 episode reward: -27.61449779304712 eps: 0.1 avg reward (last 100): 68.70479398245637\n",
      "episode: 8800 episode reward: 223.6402420833001 eps: 0.1 avg reward (last 100): 88.42711026475601\n",
      "episode: 8900 episode reward: 0.0 eps: 0.1 avg reward (last 100): 51.37131124199692\n",
      "episode: 9000 episode reward: 39.639093883793976 eps: 0.1 avg reward (last 100): 27.472639743770248\n",
      "episode: 9100 episode reward: -10.991365081379627 eps: 0.1 avg reward (last 100): 31.830105554668044\n",
      "episode: 9200 episode reward: -137.27791378325128 eps: 0.1 avg reward (last 100): 60.263872091306666\n",
      "episode: 9300 episode reward: -73.42457955099417 eps: 0.1 avg reward (last 100): 121.16122673076701\n",
      "episode: 9400 episode reward: 123.58763668987922 eps: 0.1 avg reward (last 100): 57.58106280216118\n",
      "episode: 9500 episode reward: 22.791201562364222 eps: 0.1 avg reward (last 100): 41.79151450844586\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}