{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import json\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from gym import wrappers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.StockTradingEnv import StockTradingEnv\n",
    "from networks.DQNac import MyModel\n",
    "from networks.DQNac import StockActor, StockCritic, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[2.08714200e-04 2.10714200e-04 2.15285800e-04 2.12142800e-04\n",
      "    2.11714200e-04 2.12285800e-04]\n",
      "   [2.10285800e-04 2.18571400e-04 2.16428600e-04 2.13714200e-04\n",
      "    2.15142800e-04 2.14714200e-04]\n",
      "   [2.05000000e-04 2.10285800e-04 2.09714200e-04 2.10142800e-04\n",
      "    2.09285800e-04 1.97142800e-04]\n",
      "   [2.09571400e-04 2.18142800e-04 2.12142800e-04 2.11000000e-04\n",
      "    2.14285800e-04 2.10571400e-04]\n",
      "   [2.83235684e-02 3.38649377e-02 2.79826112e-02 2.61181034e-02\n",
      "    1.83288939e-02 2.09841877e-02]\n",
      "   [4.65661288e-06 4.65661288e-06 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/AAPL.csv')\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# The algorithms require a vectorized environment to run\n",
    "env = StockTradingEnv(df)\n",
    "\n",
    "base_action = np.array([0,0.5])\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(5):\n",
    "    action, _states = (base_action, None)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "#print(obs)\n",
    "obs = env.reset()\n",
    "obs=np.expand_dims(obs,axis=0)\n",
    "obs=np.expand_dims(obs,axis=0)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[2.08714200e-04 2.10714200e-04 2.15285800e-04 2.12142800e-04\n",
      "    2.11714200e-04 2.12285800e-04]\n",
      "   [2.10285800e-04 2.18571400e-04 2.16428600e-04 2.13714200e-04\n",
      "    2.15142800e-04 2.14714200e-04]\n",
      "   [2.05000000e-04 2.10285800e-04 2.09714200e-04 2.10142800e-04\n",
      "    2.09285800e-04 1.97142800e-04]\n",
      "   [2.09571400e-04 2.18142800e-04 2.12142800e-04 2.11000000e-04\n",
      "    2.14285800e-04 2.10571400e-04]\n",
      "   [2.83235684e-02 3.38649377e-02 2.79826112e-02 2.61181034e-02\n",
      "    1.83288939e-02 2.09841877e-02]\n",
      "   [4.65661288e-06 4.65661288e-06 0.00000000e+00 0.00000000e+00\n",
      "    0.00000000e+00 0.00000000e+00]]]]\n",
      "[[[[-0.00788789]\n",
      "   [-0.00697928]]]]\n"
     ]
    }
   ],
   "source": [
    "mod = MyModel(32, (1,4), (1,1))\n",
    "\n",
    "print(obs)\n",
    "z = mod.predict(obs)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer stock_actor is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer stock_actor_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "output_action\n",
      "tf.Tensor([[0.000742]], shape=(1, 1), dtype=float32)\n",
      " \n",
      "output_action_tar\n",
      "tf.Tensor([[-0.00378864]], shape=(1, 1), dtype=float32)\n",
      " \n",
      "output_action2\n",
      "tf.Tensor(0.00074200303, shape=(), dtype=float32)\n",
      " \n",
      "output_action_tar2\n",
      "tf.Tensor([[-0.00378864]], shape=(1, 1), dtype=float32)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "act = StockActor()\n",
    "act_target = StockActor()\n",
    "output_action = act(obs)\n",
    "output_action_tar = act_target(obs)\n",
    "print(\"output_action\")\n",
    "print(output_action)\n",
    "print(\" \")\n",
    "print(\"output_action_tar\")\n",
    "print(output_action_tar)\n",
    "print(\" \")\n",
    "output_action = act(obs)\n",
    "output_action_tar = act_target(obs)\n",
    "print(\"output_action2\")\n",
    "print(tf.squeeze(output_action))\n",
    "print(\" \")\n",
    "print(\"output_action_tar2\")\n",
    "print(output_action_tar)\n",
    "print(\" \")\n",
    "\n",
    "#act.model.summary()\n",
    "#print(act.model.layers[3].get_weights())\n",
    "#act.model.layers[3].set_weights([np.array([[-0.3]], dtype=np.float32), np.array([0.1], dtype=np.float32)])\n",
    "#print(act.model.layers[3].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer stock_actor_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.02222121]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "gamma = 0.99\n",
    "max_experiences = 1000\n",
    "min_experiences = 365\n",
    "batch_size = 32\n",
    "lr = 1e-2\n",
    "D = DQN( gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "observations = env.reset()\n",
    "observations = np.expand_dims(observations,axis=0)\n",
    "observations = np.expand_dims(observations,axis=0)\n",
    "D.act(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = act.model.trainable_variables\n",
    "print(a)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" hjkbedfhugqdfhiqdfbhiqdf\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "b = act.trainable_variables\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer stock_critic_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer stock_critic_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "output\n",
      "tf.Tensor([[-0.00270959]], shape=(1, 1), dtype=float32)\n",
      " \n",
      "output_tar\n",
      "tf.Tensor([[-0.00679019]], shape=(1, 1), dtype=float32)\n",
      " \n",
      "output2\n",
      "tf.Tensor([[-0.00154407]], shape=(1, 1), dtype=float32)\n",
      " \n",
      "output_tar2\n",
      "tf.Tensor([[-0.00679019]], shape=(1, 1), dtype=float32)\n",
      " \n",
      "[<tf.Variable 'stock_critic_2/my_model_7/conv2d_14/kernel:0' shape=(1, 4, 6, 32) dtype=float32, numpy=\n",
      "array([[[[-1.93698958e-01, -7.31249899e-02,  3.98261249e-02,\n",
      "          -1.62185088e-01, -1.48793072e-01,  8.38856399e-03,\n",
      "          -1.45908505e-01, -5.62080741e-03,  4.34512198e-02,\n",
      "           7.11874217e-02,  1.92348287e-01,  1.00856438e-01,\n",
      "          -6.97050393e-02,  1.29638448e-01, -1.31197736e-01,\n",
      "           1.69876114e-01, -1.08596154e-01, -7.73834586e-02,\n",
      "           1.30830541e-01,  3.42793167e-02, -5.96470535e-02,\n",
      "          -7.99161866e-02,  1.53553054e-01, -1.79690674e-01,\n",
      "           2.13679075e-02, -1.02614686e-01,  1.04048356e-01,\n",
      "          -1.76764220e-01, -6.09942675e-02, -7.08257556e-02,\n",
      "          -1.36616826e-03, -1.56424552e-01],\n",
      "         [-3.22743654e-02,  7.22704083e-02, -1.12967692e-01,\n",
      "           1.29100099e-01,  2.79826969e-02, -9.67363864e-02,\n",
      "          -8.57706591e-02, -1.13209486e-02,  8.95239860e-02,\n",
      "          -4.45422679e-02, -3.19622606e-02,  2.70750076e-02,\n",
      "           6.49076253e-02,  1.61649361e-01, -1.02141373e-01,\n",
      "          -7.02828169e-03, -9.07228887e-03,  4.42654490e-02,\n",
      "           3.60335410e-03, -1.32054403e-01, -1.04914255e-01,\n",
      "          -6.06527925e-02,  1.25887915e-01,  1.71844915e-01,\n",
      "           1.59990445e-01,  1.28118396e-02,  1.02734253e-01,\n",
      "          -8.40746611e-02,  5.72646707e-02, -7.06970543e-02,\n",
      "          -1.36532754e-01,  1.67883947e-01],\n",
      "         [ 2.69067585e-02,  9.33397561e-02, -1.72823876e-01,\n",
      "          -8.48482922e-02,  1.66219994e-01,  1.55768543e-02,\n",
      "           7.03095347e-02,  5.82302064e-02, -6.03322387e-02,\n",
      "           1.80781707e-01, -9.46120769e-02,  8.56147259e-02,\n",
      "           3.37155759e-02, -1.26558363e-01,  1.08607560e-02,\n",
      "           1.10694513e-01,  4.46320772e-02, -4.92139459e-02,\n",
      "           1.49253011e-02,  1.06325462e-01, -5.25400937e-02,\n",
      "           1.15465224e-02, -1.08994387e-01, -1.50119081e-01,\n",
      "           1.27955899e-01,  7.36462921e-02, -1.97178155e-01,\n",
      "          -1.53464273e-01,  6.32088333e-02, -2.23722756e-02,\n",
      "          -4.53052968e-02, -5.70052862e-02],\n",
      "         [ 5.79089969e-02,  7.33994097e-02, -5.87497950e-02,\n",
      "           1.61199257e-01,  8.19914788e-02, -4.03915346e-03,\n",
      "          -6.00487888e-02,  1.67235091e-01, -9.96121988e-02,\n",
      "          -1.23534098e-01,  1.23043314e-01,  9.76960212e-02,\n",
      "          -7.33257830e-03,  3.02105546e-02,  5.74143082e-02,\n",
      "          -5.53692132e-02,  5.95494658e-02,  5.90903312e-02,\n",
      "           1.75450444e-02, -1.67254761e-01, -9.51950923e-02,\n",
      "           6.29969984e-02,  1.49362043e-01, -1.42035335e-01,\n",
      "          -1.18915290e-01,  1.83159515e-01,  9.70598310e-02,\n",
      "          -9.73193571e-02, -2.45149136e-02, -1.58158585e-01,\n",
      "           1.51814178e-01,  4.49492186e-02],\n",
      "         [ 7.73882121e-02, -1.65696129e-01, -3.23420763e-03,\n",
      "          -1.63855419e-01, -4.41549867e-02, -9.61512327e-02,\n",
      "          -2.25956142e-02,  8.25264901e-02, -1.85338572e-01,\n",
      "          -1.10276185e-01, -1.78983167e-01, -2.36320049e-02,\n",
      "           1.94779739e-01, -6.18529767e-02, -1.84733763e-01,\n",
      "          -9.28847194e-02,  1.73158199e-02,  1.92836985e-01,\n",
      "          -9.92348567e-02,  1.29544929e-01,  1.95959970e-01,\n",
      "           4.87393141e-03, -8.92373994e-02, -1.68974295e-01,\n",
      "           1.58579931e-01,  1.24065921e-01, -1.07775778e-02,\n",
      "           1.63936839e-01,  1.19290158e-01,  1.40462667e-02,\n",
      "           5.32437116e-02, -1.16206780e-01],\n",
      "         [ 1.33205608e-01,  6.32544905e-02,  1.96447775e-01,\n",
      "           1.33519486e-01,  1.41665936e-02,  4.34119105e-02,\n",
      "           5.57523817e-02,  1.37006447e-01,  1.22489929e-02,\n",
      "          -1.35954291e-01, -9.43463817e-02,  1.43146709e-01,\n",
      "           3.14602852e-02, -1.39349550e-02,  6.02673441e-02,\n",
      "           1.01106599e-01, -1.38953984e-01,  3.33055854e-04,\n",
      "           7.82441050e-02, -1.12348437e-01,  5.06941229e-02,\n",
      "           2.94686556e-02, -5.26188761e-02,  2.01397687e-02,\n",
      "           1.92473367e-01,  1.30632207e-01, -1.03096992e-01,\n",
      "           1.65222898e-01, -1.79445401e-01, -1.15344666e-01,\n",
      "          -1.94155216e-01, -7.69673735e-02]],\n",
      "\n",
      "        [[-1.64963275e-01,  1.27961621e-01,  1.60795107e-01,\n",
      "          -8.26802626e-02, -1.66618586e-01, -1.56264722e-01,\n",
      "          -4.44595665e-02, -8.30400735e-02,  8.78441483e-02,\n",
      "          -5.00538349e-02,  9.29236412e-03, -1.65858358e-01,\n",
      "           3.40132862e-02, -1.46058947e-02, -1.87428728e-01,\n",
      "          -7.11476207e-02,  9.69691426e-02, -1.78126454e-01,\n",
      "           2.89836079e-02,  2.87749469e-02, -1.73319921e-01,\n",
      "          -4.70460057e-02, -1.92771524e-01, -1.17572241e-01,\n",
      "           3.94692868e-02,  1.04635492e-01,  1.76188633e-01,\n",
      "           8.82398933e-02, -6.65014833e-02,  2.08977163e-02,\n",
      "          -4.65669185e-02, -1.21802956e-01],\n",
      "         [ 4.51912284e-02, -4.26681638e-02, -1.62249804e-01,\n",
      "          -7.40985200e-02, -2.20700502e-02,  1.06587812e-01,\n",
      "          -1.21997878e-01, -7.59672746e-02, -9.95607600e-02,\n",
      "          -1.57435164e-01, -1.61997408e-01,  6.35920316e-02,\n",
      "          -8.06647539e-02,  9.61659104e-02, -1.55965641e-01,\n",
      "           3.69511545e-03, -1.06965996e-01,  8.13440233e-02,\n",
      "          -1.44487619e-01, -1.44380987e-01,  6.58907443e-02,\n",
      "          -4.09030914e-03, -7.14344382e-02,  3.81532460e-02,\n",
      "          -1.45655453e-01, -1.91031888e-01, -1.02599382e-01,\n",
      "           1.90032080e-01,  8.54400843e-02, -1.26651555e-01,\n",
      "           5.57696670e-02,  1.05912685e-02],\n",
      "         [-1.07857101e-01,  1.90584585e-01,  8.47539157e-02,\n",
      "           1.75339267e-01, -1.53142124e-01, -9.78281945e-02,\n",
      "          -5.03845215e-02,  1.51777044e-01, -1.86134934e-01,\n",
      "          -1.15666397e-01, -1.12197429e-01, -1.05313860e-01,\n",
      "           1.59236938e-02,  2.96418965e-02, -1.40929356e-01,\n",
      "           5.63098490e-03,  8.91715139e-02,  6.93532377e-02,\n",
      "          -1.39573798e-01,  6.90808147e-02, -9.76860821e-02,\n",
      "           4.42345440e-03, -6.45497888e-02, -1.28603250e-01,\n",
      "           7.74153024e-02, -6.43319339e-02,  1.28385350e-01,\n",
      "           8.07984918e-02, -1.14485726e-01, -7.95643777e-02,\n",
      "           1.37746766e-01, -9.33185741e-02],\n",
      "         [-1.77666545e-03, -6.75631166e-02, -8.78009424e-02,\n",
      "           9.64391083e-02,  1.51586339e-01, -2.96595991e-02,\n",
      "           9.78995711e-02,  1.48084238e-01, -1.54656738e-01,\n",
      "          -4.58967388e-02,  1.77397281e-02,  8.51331800e-02,\n",
      "           1.72466382e-01, -9.99726802e-02, -5.84934205e-02,\n",
      "           8.57210010e-02, -2.76113749e-02, -1.59015492e-01,\n",
      "           1.02595165e-01, -1.54486209e-01, -9.43959281e-02,\n",
      "           1.74036577e-01, -1.35541990e-01, -1.85977533e-01,\n",
      "          -6.51077479e-02,  1.61569417e-02, -1.55778080e-02,\n",
      "           3.98997962e-03,  1.38659522e-01, -6.20200932e-02,\n",
      "           6.47023767e-02, -1.55082881e-01],\n",
      "         [ 1.20649531e-01,  1.06179729e-01,  5.66527992e-02,\n",
      "           1.39692709e-01,  1.44055292e-01,  9.22007859e-03,\n",
      "           2.28282362e-02, -1.18572481e-01, -1.73560992e-01,\n",
      "          -1.18193723e-01,  1.91458687e-01, -1.04907677e-01,\n",
      "           1.62928849e-02, -5.58325201e-02, -7.66102597e-02,\n",
      "          -1.15653180e-01, -2.44911313e-02, -1.32517815e-01,\n",
      "          -8.91430452e-02, -1.12019457e-01, -1.23462334e-01,\n",
      "          -1.14741519e-01, -1.11513227e-01, -1.06558248e-01,\n",
      "           1.51592493e-02, -8.83365870e-02, -5.57532310e-02,\n",
      "           1.05799004e-01, -1.75179988e-02,  1.70876667e-01,\n",
      "          -7.25125521e-02,  1.79974362e-01],\n",
      "         [ 1.61998734e-01, -1.00679331e-01,  2.35683322e-02,\n",
      "           5.42954355e-02, -1.36682540e-01, -1.07572034e-01,\n",
      "          -6.24756962e-02, -7.75248110e-02,  1.69704720e-01,\n",
      "           5.44744879e-02,  1.48026094e-01, -9.60933045e-02,\n",
      "          -1.54798850e-01, -1.38788223e-02, -6.88656718e-02,\n",
      "           2.68250704e-04, -1.71141759e-01,  5.80979437e-02,\n",
      "           9.09836739e-02,  6.03723973e-02, -1.12183072e-01,\n",
      "           3.22357118e-02,  2.65053511e-02, -1.81910336e-01,\n",
      "           1.20576158e-01,  1.62919179e-01, -6.84492886e-02,\n",
      "          -5.14166057e-02, -4.45682257e-02, -1.22097164e-01,\n",
      "           9.01867300e-02,  1.71629086e-01]],\n",
      "\n",
      "        [[ 2.12317258e-02, -5.09877652e-02,  1.93814933e-03,\n",
      "          -1.86558515e-01,  9.63413715e-03,  2.60924399e-02,\n",
      "           5.03948033e-02,  1.85045972e-01,  8.06529969e-02,\n",
      "          -1.55316979e-01, -1.18575566e-01,  9.81346816e-02,\n",
      "          -9.35095623e-02, -3.80322635e-02,  9.01558846e-02,\n",
      "           1.96962729e-01,  1.13640562e-01, -1.32389575e-01,\n",
      "           7.89962858e-02,  1.12751439e-01,  1.55713692e-01,\n",
      "           1.67813227e-01,  3.01792026e-02,  1.29352808e-02,\n",
      "          -1.64425075e-01, -1.91652924e-02, -1.61406875e-01,\n",
      "          -9.08082053e-02, -1.59297571e-01,  1.68398097e-01,\n",
      "           1.81984857e-01, -5.04148901e-02],\n",
      "         [ 7.59781748e-02,  1.05765417e-01, -3.34878713e-02,\n",
      "           1.17905334e-01, -1.71540707e-01,  1.67552426e-01,\n",
      "          -1.11012541e-01,  3.30050439e-02,  1.97363988e-01,\n",
      "          -1.46012470e-01, -1.70758784e-01,  1.98366359e-01,\n",
      "          -3.13315392e-02, -1.50823116e-01, -1.13517977e-01,\n",
      "           1.30556524e-02, -1.93408430e-02, -1.50130212e-01,\n",
      "           1.24813244e-01,  6.35998100e-02, -1.49929702e-01,\n",
      "          -1.47042185e-01, -9.11802053e-05, -1.46028161e-01,\n",
      "           9.34796780e-02, -1.41636670e-01, -5.29646873e-03,\n",
      "          -1.27815694e-01, -1.91692591e-01,  6.35015219e-02,\n",
      "          -1.11560456e-01, -1.81262285e-01],\n",
      "         [ 1.44705519e-01, -1.15142211e-01, -1.31124556e-01,\n",
      "          -1.76858902e-01,  1.11750588e-01, -1.55721694e-01,\n",
      "          -1.50822103e-03,  1.92808315e-01, -9.56881046e-03,\n",
      "           6.16909713e-02, -1.10673472e-01,  1.50648370e-01,\n",
      "           3.05159390e-02, -1.16803817e-01,  6.88394904e-03,\n",
      "          -2.27356404e-02, -1.49117887e-01,  8.09945017e-02,\n",
      "           1.57966465e-02, -3.13336253e-02,  4.83187139e-02,\n",
      "          -9.97033417e-02,  1.52334109e-01,  1.19493112e-01,\n",
      "          -3.82038802e-02, -1.64705127e-01,  9.33101326e-02,\n",
      "          -2.92251855e-02,  3.90450060e-02, -4.29511070e-02,\n",
      "          -9.79661793e-02, -5.50234616e-02],\n",
      "         [ 1.15866110e-01, -2.53732800e-02,  1.73537448e-01,\n",
      "          -5.41362911e-02, -6.05113506e-02,  1.97304174e-01,\n",
      "          -1.32198691e-01,  1.98391035e-01, -3.44343036e-02,\n",
      "           5.87783605e-02,  2.62789279e-02,  1.54743895e-01,\n",
      "          -5.69821149e-02, -8.35606083e-02,  1.59927711e-01,\n",
      "          -8.52574632e-02, -1.59484595e-01,  1.65629253e-01,\n",
      "           1.92532793e-01,  1.53008357e-01,  1.90029338e-01,\n",
      "           7.24877268e-02, -1.78941667e-01, -1.39406070e-01,\n",
      "          -9.21586454e-02,  7.51845986e-02,  4.85862494e-02,\n",
      "           8.23040158e-02, -1.36161059e-01,  1.44447014e-01,\n",
      "          -9.07936171e-02, -9.37242359e-02],\n",
      "         [-5.78260422e-02,  1.42589077e-01, -1.63723782e-01,\n",
      "           5.45205325e-02,  4.30575907e-02, -6.39812648e-03,\n",
      "          -9.58376974e-02, -6.56826198e-02, -4.09706086e-02,\n",
      "          -6.90401793e-02,  1.56529382e-01,  5.37530631e-02,\n",
      "           8.76749009e-02, -1.48434833e-01, -1.14241779e-01,\n",
      "          -1.35483161e-01, -8.18765461e-02, -5.32834083e-02,\n",
      "           5.23375124e-02, -2.69356072e-02, -2.50628740e-02,\n",
      "          -7.90058449e-02,  6.48591965e-02,  9.02086645e-02,\n",
      "          -1.56230927e-02,  1.78970471e-01,  1.40964940e-01,\n",
      "          -1.66839942e-01,  2.61437893e-02, -1.22648537e-01,\n",
      "           1.13716111e-01,  7.83578008e-02],\n",
      "         [ 9.57104564e-03, -1.02204092e-01, -1.28970161e-01,\n",
      "           1.06563494e-01,  1.04009047e-01, -2.64096260e-03,\n",
      "          -1.00242257e-01,  9.23209935e-02,  1.42784134e-01,\n",
      "          -1.14270195e-01,  1.12605378e-01,  1.85912952e-01,\n",
      "           1.63444623e-01,  1.94434121e-01, -1.07711680e-01,\n",
      "           1.20639488e-01, -1.29640192e-01, -1.71389729e-01,\n",
      "           9.54925269e-02, -8.07250068e-02, -7.78853819e-02,\n",
      "          -5.41121811e-02, -6.06907308e-02, -3.25616151e-02,\n",
      "           7.23371059e-02,  1.13440618e-01, -1.79446623e-01,\n",
      "          -4.58818227e-02,  1.05008140e-01, -1.28728718e-01,\n",
      "           1.25335917e-01,  2.52200961e-02]],\n",
      "\n",
      "        [[-8.83507058e-02,  1.26055792e-01, -1.38916641e-01,\n",
      "           6.97162300e-02,  1.48040280e-01, -1.39889419e-01,\n",
      "          -3.29659581e-02, -1.57539040e-01, -4.03521061e-02,\n",
      "          -1.80230916e-01,  1.60425469e-01, -1.57263026e-01,\n",
      "           7.83622116e-02, -1.68520734e-01,  8.24256390e-02,\n",
      "           1.16344675e-01, -1.88088417e-03, -1.96186259e-01,\n",
      "          -1.92825377e-01,  8.74876529e-02, -1.57164976e-01,\n",
      "           4.18977141e-02,  4.95167673e-02, -4.56063747e-02,\n",
      "           1.23990461e-01, -1.49176195e-01,  7.24942237e-02,\n",
      "           1.11555755e-02,  2.32897103e-02, -1.29459709e-01,\n",
      "          -3.71500552e-02, -1.18014291e-01],\n",
      "         [-1.00045726e-01,  1.36329725e-01,  6.24631494e-02,\n",
      "          -1.24938302e-01,  1.13837674e-01,  1.56315878e-01,\n",
      "          -1.16388917e-01, -8.80573466e-02,  5.29159158e-02,\n",
      "           2.51926631e-02, -3.65298092e-02,  5.77628464e-02,\n",
      "          -8.31290781e-02,  8.88044387e-02,  1.75334856e-01,\n",
      "          -6.70787245e-02, -1.73850924e-01, -2.19137967e-03,\n",
      "           1.41204596e-02,  1.93329617e-01, -7.88772404e-02,\n",
      "          -1.33022815e-01, -1.24706812e-01, -8.35302919e-02,\n",
      "          -2.69841999e-02,  8.91463608e-02,  2.00945735e-02,\n",
      "           8.87496918e-02, -1.55614585e-01,  2.92678624e-02,\n",
      "           2.83092111e-02, -1.55847505e-01],\n",
      "         [ 4.70541567e-02,  1.31382987e-01, -1.05961964e-01,\n",
      "          -1.59917623e-02, -1.73036709e-01,  8.28626305e-02,\n",
      "          -1.98316380e-01,  1.39323935e-01,  1.67826787e-01,\n",
      "          -1.41087294e-01,  3.80260944e-02, -1.24353342e-01,\n",
      "           1.79737553e-01, -6.23685867e-02,  3.39491963e-02,\n",
      "           1.00034773e-02,  6.33555651e-03, -8.85692611e-02,\n",
      "           7.73943514e-02, -1.27775803e-01, -1.41770542e-02,\n",
      "           1.28586128e-01, -1.60230786e-01, -1.96331486e-01,\n",
      "          -8.43330100e-02,  1.09565839e-01, -1.83532625e-01,\n",
      "           3.40762883e-02, -1.21437788e-01,  4.65136319e-02,\n",
      "          -7.61134550e-02,  1.53275475e-01],\n",
      "         [-5.44923097e-02,  1.06200725e-02,  1.47817269e-01,\n",
      "          -8.10671598e-02, -1.36510208e-01,  8.14779103e-03,\n",
      "          -5.99664599e-02,  2.92067081e-02, -1.79734200e-01,\n",
      "           1.62842721e-02,  7.18472153e-02,  2.72118598e-02,\n",
      "          -1.17106415e-01, -2.16134191e-02, -1.59069970e-01,\n",
      "          -1.77885965e-01, -1.59656346e-01, -1.40511096e-02,\n",
      "          -1.46058843e-01, -1.85469255e-01,  5.52733690e-02,\n",
      "           1.58854857e-01, -1.24866307e-01, -1.40848458e-01,\n",
      "          -7.88163245e-02, -5.96570522e-02,  1.62307635e-01,\n",
      "          -1.35684907e-01,  1.63198844e-01,  1.42747179e-01,\n",
      "          -1.18085437e-01, -1.54767677e-01],\n",
      "         [ 1.80628747e-02, -4.44700867e-02,  1.57892182e-01,\n",
      "          -1.07729159e-01,  6.93320483e-02,  4.95760739e-02,\n",
      "           6.84657842e-02,  3.38978022e-02,  4.30465490e-02,\n",
      "          -4.57288623e-02,  7.65490979e-02, -4.43495214e-02,\n",
      "           6.76108748e-02, -1.46319747e-01, -1.17492974e-02,\n",
      "          -3.51542234e-02,  8.07805806e-02,  1.09642401e-01,\n",
      "          -1.95380703e-01,  1.91209778e-01, -4.97502983e-02,\n",
      "          -1.42021030e-01, -5.33645004e-02, -1.94831967e-01,\n",
      "           1.82591930e-01,  1.41412005e-01, -1.42607093e-02,\n",
      "          -1.78762481e-01,  1.05197534e-01, -1.40105039e-01,\n",
      "          -1.25662714e-01,  1.37370750e-01],\n",
      "         [-1.94970295e-01, -1.85334876e-01,  1.38522312e-01,\n",
      "          -1.47249758e-01, -1.70896724e-01,  1.25251308e-01,\n",
      "          -1.11668743e-01,  2.29758471e-02, -3.74199152e-02,\n",
      "           6.99099898e-03, -1.40643060e-01,  7.13927597e-02,\n",
      "           1.24499187e-01, -1.12899721e-01, -2.16711611e-02,\n",
      "           1.02873608e-01,  1.98286548e-01,  1.23667523e-01,\n",
      "          -9.08857957e-02, -6.51027262e-02, -1.59211934e-01,\n",
      "          -7.12750852e-02, -1.45375878e-02,  3.35501581e-02,\n",
      "          -1.98602691e-01, -6.80880994e-02, -9.58356187e-02,\n",
      "          -8.73810574e-02,  1.06546581e-02,  1.01478115e-01,\n",
      "          -8.25929567e-02, -9.01336968e-03]]]], dtype=float32)>, <tf.Variable 'stock_critic_2/my_model_7/conv2d_14/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_2/my_model_7/batch_normalization_14/gamma:0' shape=(32,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_2/my_model_7/batch_normalization_14/beta:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_2/my_model_7/conv2d_15/kernel:0' shape=(1, 2, 32, 1) dtype=float32, numpy=\n",
      "array([[[[-2.47424826e-01],\n",
      "         [ 1.94266558e-01],\n",
      "         [ 5.86708188e-02],\n",
      "         [ 6.45335615e-02],\n",
      "         [ 2.64760137e-01],\n",
      "         [-2.06768304e-01],\n",
      "         [-1.25495613e-01],\n",
      "         [ 1.22574866e-02],\n",
      "         [ 9.78208184e-02],\n",
      "         [ 9.22381878e-05],\n",
      "         [-2.00778246e-03],\n",
      "         [-2.31280535e-01],\n",
      "         [-1.27277598e-01],\n",
      "         [-1.41245306e-01],\n",
      "         [-1.40388355e-01],\n",
      "         [-1.58741474e-01],\n",
      "         [ 1.11970425e-01],\n",
      "         [-5.47412038e-03],\n",
      "         [-1.48110986e-02],\n",
      "         [-2.91781873e-01],\n",
      "         [ 1.52388126e-01],\n",
      "         [ 2.35301316e-01],\n",
      "         [ 8.85625780e-02],\n",
      "         [ 2.53009140e-01],\n",
      "         [ 2.15670824e-01],\n",
      "         [ 1.73894167e-01],\n",
      "         [ 1.26552045e-01],\n",
      "         [ 1.23464406e-01],\n",
      "         [-1.75737828e-01],\n",
      "         [ 2.95086980e-01],\n",
      "         [-1.77634746e-01],\n",
      "         [-2.05318660e-01]],\n",
      "\n",
      "        [[ 1.84171557e-01],\n",
      "         [-2.31740892e-01],\n",
      "         [ 2.51379311e-02],\n",
      "         [ 8.10911953e-02],\n",
      "         [-2.82617509e-01],\n",
      "         [-1.08823180e-01],\n",
      "         [-4.29569185e-02],\n",
      "         [ 8.92508030e-03],\n",
      "         [-2.57765621e-01],\n",
      "         [ 5.65043390e-02],\n",
      "         [-8.65171999e-02],\n",
      "         [ 1.92549407e-01],\n",
      "         [-3.71063650e-02],\n",
      "         [-2.09638000e-01],\n",
      "         [-6.63375407e-02],\n",
      "         [-2.70496607e-01],\n",
      "         [ 1.04387045e-01],\n",
      "         [ 2.74828255e-01],\n",
      "         [-1.35224506e-01],\n",
      "         [-1.90437630e-01],\n",
      "         [ 2.37633169e-01],\n",
      "         [ 1.38761222e-01],\n",
      "         [-1.95876598e-01],\n",
      "         [-2.58211195e-02],\n",
      "         [-1.61156267e-01],\n",
      "         [ 1.50643468e-01],\n",
      "         [ 1.67035460e-02],\n",
      "         [-3.50530744e-02],\n",
      "         [-2.85772502e-01],\n",
      "         [-2.52579033e-01],\n",
      "         [-2.41441324e-01],\n",
      "         [-2.30825067e-01]]]], dtype=float32)>, <tf.Variable 'stock_critic_2/my_model_7/conv2d_15/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Variable 'stock_critic_2/my_model_7/batch_normalization_15/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>, <tf.Variable 'stock_critic_2/my_model_7/batch_normalization_15/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Variable 'stock_critic_2/dense_14/kernel:0' shape=(3, 10) dtype=float32, numpy=\n",
      "array([[-0.03355869,  0.49787492,  0.39386627,  0.22331041, -0.11725281,\n",
      "        -0.5403902 , -0.40410483,  0.04341761,  0.30474335,  0.3552154 ],\n",
      "       [ 0.33564514, -0.66824055,  0.21725842, -0.53236115,  0.17396982,\n",
      "        -0.37265974,  0.7388386 , -0.43450996, -0.69319665,  0.3073756 ],\n",
      "       [-0.2765984 , -0.28901508, -0.16919674, -0.7135928 , -0.44398934,\n",
      "        -0.06701585,  0.21545567,  0.43611935,  0.17272928,  0.267936  ]],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_2/dense_14/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'stock_critic_2/dense_15/kernel:0' shape=(10, 10) dtype=float32, numpy=\n",
      "array([[-0.17379285, -0.28745502, -0.69388944,  0.22791006, -0.6422369 ,\n",
      "         0.29136524,  0.21630053,  0.16638252, -0.00421351, -0.40928972],\n",
      "       [-0.07049692, -0.18837258, -0.512992  ,  0.47369424, -0.45082933,\n",
      "         0.28833586, -0.5960152 ,  0.22727892, -0.0478029 , -0.25572702],\n",
      "       [ 0.17452587,  0.04912268,  0.63708097, -0.3580149 ,  0.5955629 ,\n",
      "        -0.40640348, -0.13977611,  0.07140163, -0.00275818, -0.18023267],\n",
      "       [ 0.0573304 ,  0.43444082,  0.0227975 , -0.24932042,  0.2958867 ,\n",
      "         0.27469254,  0.35340863,  0.23071252, -0.20895948,  0.28063443],\n",
      "       [-0.06388236,  0.0636712 , -0.14396553,  0.24054715, -0.24590701,\n",
      "        -0.16717352,  0.48705018, -0.04066359,  0.41486952, -0.45936266],\n",
      "       [ 0.47878724,  0.46232036, -0.01981299,  0.18175934,  0.18477182,\n",
      "         0.6734414 ,  0.62438583, -0.10088532,  0.48985136,  0.27874216],\n",
      "       [-0.2447069 , -0.4876005 , -0.15069564,  0.04981053, -0.38425967,\n",
      "        -0.18687563,  0.18503846, -0.19502337,  0.0355825 , -0.16589585],\n",
      "       [-0.21514982, -0.05135289,  0.22307926,  0.20983468, -0.21462348,\n",
      "         0.01477751, -0.43701673,  0.07324186, -0.58968014,  0.32290795],\n",
      "       [ 0.35842937,  0.33919695, -0.0847839 , -0.04671587, -0.4576568 ,\n",
      "        -0.5434822 ,  0.5005032 , -0.00723983, -0.20046924, -0.30534035],\n",
      "       [-0.0233623 , -0.3975831 , -0.49718094, -0.0371075 ,  0.3576785 ,\n",
      "         0.36482534,  0.30353636,  0.28648445,  0.23854108,  0.27099413]],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_2/dense_15/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'stock_critic_2/dense_16/kernel:0' shape=(10, 1) dtype=float32, numpy=\n",
      "array([[0.9224642 ],\n",
      "       [0.05175923],\n",
      "       [0.13860759],\n",
      "       [0.5881288 ],\n",
      "       [0.34259355],\n",
      "       [0.44558197],\n",
      "       [0.03594284],\n",
      "       [0.45264658],\n",
      "       [0.36398688],\n",
      "       [0.5453248 ]], dtype=float32)>, <tf.Variable 'stock_critic_2/dense_16/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = MyModel(32, (1,4), (1,1))\n",
    "#output=tf.squeeze(model(obs))\n",
    "\n",
    "crit = StockCritic()\n",
    "crit_tar = StockCritic()\n",
    "crit.action = tf.squeeze(output_action)\n",
    "crit_tar.action = tf.squeeze(output_action)\n",
    "output = crit(obs)\n",
    "output_tar = crit_tar(obs)\n",
    "print(\"output\")\n",
    "print(output)\n",
    "print(\" \")\n",
    "print(\"output_tar\")\n",
    "print(output_tar)\n",
    "print(\" \")\n",
    "crit.action = tf.squeeze(output_action_tar)\n",
    "crit_tar.action = tf.squeeze(output_action)\n",
    "output = crit(obs)\n",
    "output_tar = crit_tar(obs)\n",
    "print(\"output2\")\n",
    "print(output)\n",
    "print(\" \")\n",
    "print(\"output_tar2\")\n",
    "print(output_tar)\n",
    "print(\" \")\n",
    "print(crit.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_action(action):\n",
    "    \n",
    "    \n",
    "    if action > 10e-3:\n",
    "        return np.array([0, action]) #buy stocks with action% of remaining balance\n",
    "    elif action < -10e-3:\n",
    "        return np.array([1, -action]) #sell action% of stocks\n",
    "    else:\n",
    "        return np.array([2, 0]) #do nothing\n",
    "\n",
    "def fill_buffer(env, DQN):\n",
    "    \n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    observations = np.expand_dims(observations,axis=0)\n",
    "    observations = np.expand_dims(observations,axis=0)\n",
    "    steps = 0\n",
    "    it=0\n",
    "    while it<DQN.min_experiences:\n",
    "        \n",
    "        #print(it)\n",
    "        #observations = env.reset()\n",
    "\n",
    "        action = DQN.act(observations) # observations is actually a single \"state\" ie past 5 days\n",
    "        action = np.tanh(tf.squeeze(action)+np.random.normal(0,0.05)) #avoiding small action values in the begining ->[2 0] to frequently\n",
    "        #print(action)\n",
    "        action = convert_action(action)\n",
    "         \n",
    "        #print(action)\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        #print(reward)\n",
    "        observations = np.expand_dims(observations,axis=0)\n",
    "        observations = np.expand_dims(observations,axis=0)\n",
    "        rewards += reward    # sum of gain_net_worth\n",
    "\n",
    "        if done :\n",
    "            print(\"DONE\")\n",
    "#             reward = -200\n",
    "            env.reset()\n",
    "        steps += 1\n",
    "        if steps >= 300: # Limiting the number of steps\n",
    "            print(\"STEPS\")\n",
    "            observations = env.reset()\n",
    "            observations = np.expand_dims(observations,axis=0)\n",
    "            observations = np.expand_dims(observations,axis=0)\n",
    "            steps = 0\n",
    "        \n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        DQN.add_experience(exp)\n",
    "        \n",
    "        it += 1\n",
    "    return \" Done \"\n",
    "\n",
    "def DDPG():\n",
    "    \n",
    "    gamma = 0.99\n",
    "    max_experiences = 1000\n",
    "    min_experiences = 365\n",
    "    batch_size = 32\n",
    "    lr = 1e-2\n",
    "    \n",
    "    M = 1\n",
    "    T = 1\n",
    " \n",
    "    D = DQN( gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "    a = fill_buffer(env, D)\n",
    "    #print(a)\n",
    "    \n",
    "    for i in range(M):\n",
    "        observations = env.reset()\n",
    "        observations = np.expand_dims(observations,axis=0)\n",
    "        observations = np.expand_dims(observations,axis=0)\n",
    "        \n",
    "        for t in range(T):\n",
    "            action = D.act(observations) # observations is actually a single \"state\" ie past 5 days\n",
    "            action = tf.squeeze(action)\n",
    "            action = convert_action(action)\n",
    "            #print(action)\n",
    "            prev_observations = observations\n",
    "            observations, reward, done, _ = env.step(action)\n",
    "            observations = np.expand_dims(observations,axis=0)\n",
    "            observations = np.expand_dims(observations,axis=0)\n",
    "            exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "            D.add_experience(exp)\n",
    "            \n",
    "            \n",
    "            ids = np.random.randint(low=0, high=len(D.experience['s']), size=D.batch_size)\n",
    "            states = np.asarray([D.experience['s'][i] for i in ids])\n",
    "            actions = np.asarray([D.experience['a'][i] for i in ids])\n",
    "            rewards = np.asarray([D.experience['r'][i] for i in ids])\n",
    "            states_next = np.asarray([D.experience['s2'][i] for i in ids])\n",
    "            dones = np.asarray([D.experience['done'][i] for i in ids])\n",
    "            \n",
    "            Y = np.zeros(batch_size)\n",
    "            Qval = np.zeros(batch_size)\n",
    "            #print(Y)\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                \n",
    "                #calcul de crit_target(s_next,action_target(s_next))\n",
    "                actj = D.act_tar(states_next[j])\n",
    "                D.crit_tar.action = tf.squeeze(actj)\n",
    "                value_next = D.crit_tar(states_next[j])\n",
    "                \n",
    "                rj = tf.squeeze(rewards[j])\n",
    "                rj = tf.dtypes.cast(rj,tf.float32)\n",
    "                next_val = tf.squeeze(value_next)\n",
    "                Y[j] = rj+ gamma* next_val                \n",
    "                \n",
    "                #calcul de crit(s,a)\n",
    "                #conversion from action[ , ] to action\n",
    "                act = actions[j]\n",
    "                if act[0]==2:\n",
    "                    act_eff=tf.constant(0,dtype=tf.float32)\n",
    "                elif act[0]==1:\n",
    "                    act_eff=tf.constant(-act[1],dtype=tf.float32)\n",
    "                else:\n",
    "                    act_eff=tf.constant(act[1],dtype=tf.float32)\n",
    "                \n",
    "                \n",
    "                D.crit.action = tf.squeeze(act_eff)\n",
    "                value = D.crit(states_next[j])\n",
    "                Qval[j] = tf.squeeze(value)\n",
    "                \n",
    "  \n",
    "            \n",
    "            \n",
    "            Y = tf.constant(Y, dtype=tf.float32)\n",
    "            Qval = tf.constant(Qval, dtype=tf.float32)\n",
    "            #update Q\n",
    "            D.updateQ(Y,Qval)            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer stock_actor_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "STEPS\n",
      "WARNING:tensorflow:Layer stock_actor_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer stock_critic_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer stock_critic_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[<tf.Variable 'stock_critic_4/my_model_11/conv2d_22/kernel:0' shape=(1, 4, 6, 32) dtype=float32, numpy=\n",
      "array([[[[ 5.48987240e-02, -4.53109294e-02,  1.42232254e-01,\n",
      "           1.53537095e-02,  1.59927294e-01, -1.88583106e-01,\n",
      "          -6.60252422e-02,  1.04401782e-01, -1.38447270e-01,\n",
      "          -1.90922618e-01, -1.55729458e-01,  1.59495756e-01,\n",
      "           8.73639733e-02,  1.33302197e-01, -1.95319876e-01,\n",
      "          -1.24068566e-01, -1.56793565e-01, -1.09872982e-01,\n",
      "           3.98665220e-02,  1.96593001e-01, -1.00279681e-01,\n",
      "          -8.72003511e-02, -1.87199697e-01, -6.04061931e-02,\n",
      "           6.46026880e-02, -1.60023272e-02, -6.29523098e-02,\n",
      "          -4.51114625e-02, -4.28944975e-02, -1.10085525e-01,\n",
      "          -2.13892162e-02, -6.41841888e-02],\n",
      "         [ 1.03209063e-01,  1.37899593e-01,  1.04748175e-01,\n",
      "          -5.37475348e-02, -1.71946838e-01,  1.46801457e-01,\n",
      "           3.75975966e-02,  1.65244654e-01, -1.33489639e-01,\n",
      "          -3.50026786e-02, -2.51531601e-04,  9.58122760e-02,\n",
      "           7.05574006e-02, -6.39068484e-02,  1.93212822e-01,\n",
      "          -4.50816602e-02, -1.56249285e-01,  1.32129386e-01,\n",
      "           1.23036250e-01, -3.80731821e-02,  6.12945408e-02,\n",
      "          -4.20096368e-02,  1.13750383e-01,  1.65521517e-01,\n",
      "           5.32380790e-02,  1.80168763e-01, -1.22003041e-01,\n",
      "           1.96968213e-01, -1.06883340e-01, -8.72542560e-02,\n",
      "          -1.34881005e-01,  1.17351413e-02],\n",
      "         [ 1.22323677e-01, -1.05429441e-01,  5.34404963e-02,\n",
      "           5.75447828e-02,  1.63906142e-01,  2.66024619e-02,\n",
      "          -6.82226270e-02,  1.84431538e-01,  4.00343984e-02,\n",
      "           1.50955006e-01,  7.66831189e-02,  1.78324118e-01,\n",
      "          -2.75419801e-02, -1.60902292e-01, -1.71044886e-01,\n",
      "           1.19337067e-01, -1.58572644e-01, -1.06600210e-01,\n",
      "           9.01227146e-02, -1.34039775e-01,  1.44708589e-01,\n",
      "          -6.12439513e-02, -4.55068052e-02, -7.35420734e-02,\n",
      "          -6.79189563e-02, -1.07966997e-01,  1.47722736e-01,\n",
      "           2.40730047e-02, -1.68794245e-01, -2.90369838e-02,\n",
      "          -1.95947602e-01,  9.90893990e-02],\n",
      "         [ 5.91145456e-03, -1.81517899e-02,  2.54370421e-02,\n",
      "           6.76040500e-02,  1.53039321e-01,  1.95816159e-04,\n",
      "           1.48318544e-01,  1.72772899e-01, -1.16012052e-01,\n",
      "           2.70504206e-02,  1.77932635e-01,  5.72858602e-02,\n",
      "           7.89106339e-02, -1.66946813e-01, -1.89173222e-01,\n",
      "          -1.98019236e-01,  4.08577174e-02, -1.25244826e-01,\n",
      "           3.69476527e-02,  4.83541936e-02,  5.34275174e-03,\n",
      "           5.57664782e-02, -7.52352774e-02, -6.74247742e-04,\n",
      "           7.37524480e-02,  5.82751185e-02,  7.40805119e-02,\n",
      "          -1.36724800e-01, -1.74662411e-01, -1.14799164e-01,\n",
      "          -5.17855585e-02,  5.99815696e-02],\n",
      "         [-9.95662808e-03, -9.46202204e-02,  1.89125016e-01,\n",
      "          -5.63521087e-02, -8.09700936e-02,  4.58664298e-02,\n",
      "           2.64489353e-02, -1.30828440e-01,  1.76386461e-01,\n",
      "          -1.13483205e-01, -1.30734503e-01,  2.99398452e-02,\n",
      "           1.81934237e-03, -1.70835227e-01,  3.21497470e-02,\n",
      "          -8.60881209e-02,  1.18417740e-02, -6.54009134e-02,\n",
      "           1.42587796e-01,  1.03542805e-02, -1.57284260e-01,\n",
      "          -9.62949023e-02,  3.40874642e-02,  6.08231872e-02,\n",
      "           1.66353419e-01, -7.71217495e-02, -5.72822988e-02,\n",
      "           7.22878426e-02, -1.71177045e-01,  3.49606723e-02,\n",
      "          -5.31616211e-02, -1.93569168e-01],\n",
      "         [-1.69937491e-01, -1.63099259e-01, -1.15806706e-01,\n",
      "          -4.33851779e-03,  5.24720699e-02,  4.21429873e-02,\n",
      "          -1.57989010e-01, -7.24986196e-04,  9.83167440e-02,\n",
      "          -1.79186285e-01, -4.47232276e-02, -2.24409550e-02,\n",
      "          -1.15283281e-01, -7.97280371e-02, -2.57089436e-02,\n",
      "          -9.28270742e-02,  1.31713748e-02,  5.07274270e-02,\n",
      "          -7.59197101e-02,  1.13946512e-01, -1.79392397e-01,\n",
      "           1.82163820e-01,  1.49359152e-01,  1.22431770e-01,\n",
      "          -1.08526185e-01, -1.69759765e-01,  1.47027865e-01,\n",
      "          -8.02079812e-02,  1.83739290e-01, -1.28511772e-01,\n",
      "           1.07382044e-01,  1.70404166e-02]],\n",
      "\n",
      "        [[-6.75074607e-02,  1.24224886e-01, -1.36091888e-01,\n",
      "          -2.34802812e-02,  1.87116936e-01, -1.49223611e-01,\n",
      "          -5.44459373e-02, -7.35009164e-02,  7.58274049e-02,\n",
      "          -1.38769567e-01, -7.41395354e-03,  6.53263479e-02,\n",
      "          -1.87775850e-01, -5.60943782e-02,  1.41270414e-01,\n",
      "          -1.69153586e-01, -3.88427377e-02,  1.78738788e-01,\n",
      "           1.36217281e-01, -1.95968166e-01,  1.96765587e-01,\n",
      "          -1.38133824e-01, -1.98266834e-01, -1.09232984e-01,\n",
      "          -1.67312071e-01, -1.57020539e-02, -1.50596321e-01,\n",
      "           1.71208307e-01,  9.20379609e-02,  1.68594494e-01,\n",
      "          -6.63958937e-02,  1.82079688e-01],\n",
      "         [-1.57928318e-01, -1.09197408e-01,  6.16184026e-02,\n",
      "           1.07448891e-01, -1.10943764e-02,  3.02697122e-02,\n",
      "          -1.67769492e-02,  1.41219929e-01, -1.59257114e-01,\n",
      "          -1.43159777e-01,  8.22110325e-02,  1.27313063e-01,\n",
      "           1.88283175e-02,  8.16701502e-02, -9.94100794e-02,\n",
      "           1.47572562e-01,  9.02265310e-03,  1.83974817e-01,\n",
      "          -8.79594386e-02,  3.42654735e-02, -1.50007099e-01,\n",
      "           1.73740223e-01,  1.36337116e-01,  1.02397054e-02,\n",
      "           1.75813898e-01, -2.09291726e-02, -1.80286765e-02,\n",
      "          -5.78199774e-02, -1.29117668e-01,  1.09428465e-02,\n",
      "          -1.12604558e-01,  9.33756381e-02],\n",
      "         [-1.48129538e-01, -1.51277542e-01, -7.76266083e-02,\n",
      "          -1.30074710e-01, -1.62265480e-01, -1.88584000e-01,\n",
      "           1.77229479e-01,  9.20985490e-02,  1.89661637e-01,\n",
      "          -2.06775963e-02,  9.39130932e-02, -1.55733764e-01,\n",
      "          -1.46825567e-01,  1.48938015e-01, -1.72605887e-01,\n",
      "          -6.11834675e-02,  4.69190031e-02,  1.49356380e-01,\n",
      "          -1.47893399e-01,  1.66606024e-01,  1.07441738e-01,\n",
      "           3.77127081e-02,  8.77930671e-02, -1.16881318e-01,\n",
      "          -8.16569924e-02, -2.49931514e-02,  1.57539442e-01,\n",
      "           9.12853628e-02, -1.24430791e-01,  3.65868360e-02,\n",
      "          -1.42157167e-01, -2.90040672e-02],\n",
      "         [-1.86918825e-02, -3.82408500e-04, -2.70569175e-02,\n",
      "           4.31086123e-03, -1.95066541e-01,  1.19305149e-01,\n",
      "           1.85977951e-01,  1.32682577e-01, -3.72897536e-02,\n",
      "           4.58503664e-02, -1.84100479e-01, -1.06792532e-01,\n",
      "           4.91046160e-02,  8.81725997e-02, -9.58437622e-02,\n",
      "          -1.42333135e-01,  1.68836609e-01, -1.81227028e-02,\n",
      "           1.30870000e-01, -4.17117923e-02, -2.47047096e-02,\n",
      "           1.58130601e-01,  6.47551268e-02, -1.80843353e-01,\n",
      "          -1.45951092e-01, -7.77629837e-02,  8.20240229e-02,\n",
      "           5.21747023e-02, -1.91582412e-01, -1.36865526e-01,\n",
      "           1.55087844e-01, -1.22852221e-01],\n",
      "         [ 7.18424171e-02,  7.39740878e-02,  5.25933355e-02,\n",
      "          -1.20557196e-01, -7.87627697e-03,  2.19563246e-02,\n",
      "           2.23114938e-02, -8.53809044e-02, -6.07752800e-03,\n",
      "          -6.70532882e-02, -1.68117777e-01, -1.78361684e-02,\n",
      "          -1.62303239e-01,  1.11947730e-01, -3.49960476e-02,\n",
      "           1.96315512e-01,  1.38601065e-02, -5.67600131e-02,\n",
      "          -6.15629405e-02,  1.58223137e-01,  1.76460072e-01,\n",
      "           2.96300054e-02, -1.93725958e-01,  6.60911202e-03,\n",
      "          -1.03467658e-01, -9.98018160e-02,  4.76028770e-02,\n",
      "           9.35172886e-02, -1.67039841e-01,  5.65293580e-02,\n",
      "          -1.86417356e-01,  8.06067437e-02],\n",
      "         [-1.25382900e-01, -4.84074801e-02,  1.05555072e-01,\n",
      "           8.77781957e-02,  1.43834725e-01, -1.63180500e-01,\n",
      "          -1.30110949e-01,  1.18365631e-01,  1.37424335e-01,\n",
      "           8.51064473e-02,  1.77977279e-01, -1.62977904e-01,\n",
      "          -2.58487463e-03,  5.71082085e-02,  1.54980704e-01,\n",
      "           7.92477280e-02,  1.07396826e-01, -9.73862410e-02,\n",
      "          -2.68470794e-02,  6.80161864e-02,  1.37125626e-01,\n",
      "           4.87324297e-02,  1.26159623e-01, -1.39796615e-01,\n",
      "           1.93310663e-01, -5.44843078e-02,  1.04175165e-01,\n",
      "           1.82927713e-01,  8.18042308e-02, -1.26384810e-01,\n",
      "          -3.26547474e-02, -1.44341245e-01]],\n",
      "\n",
      "        [[-9.95302498e-02, -7.61207938e-02,  3.75575721e-02,\n",
      "          -1.70527101e-01, -6.19336963e-02,  1.61137432e-02,\n",
      "           1.95554152e-01, -1.72702521e-01,  8.15810710e-02,\n",
      "           4.68418002e-02, -1.81332231e-04,  1.21302649e-01,\n",
      "           1.08847871e-01,  2.17017084e-02,  1.82597145e-01,\n",
      "          -1.11946657e-01, -1.92119241e-01,  1.17465004e-01,\n",
      "           4.38351482e-02,  1.78742245e-01, -4.15352434e-02,\n",
      "           1.58439532e-01, -1.35660470e-02, -3.36229801e-04,\n",
      "          -1.43169299e-01, -1.00027487e-01, -6.58722222e-03,\n",
      "          -9.51476395e-03,  1.13970980e-01,  7.43644983e-02,\n",
      "          -7.28984177e-02,  1.37276128e-01],\n",
      "         [ 8.12813193e-02, -7.62609094e-02,  1.52453229e-01,\n",
      "          -1.57036424e-01, -3.76767516e-02,  1.52339116e-01,\n",
      "           5.49291074e-03,  3.33454311e-02, -9.67241600e-02,\n",
      "           4.25090492e-02,  1.62796482e-01,  1.69446990e-01,\n",
      "          -1.74289420e-01, -1.27789631e-01, -5.16220927e-04,\n",
      "           6.79138303e-03, -1.19336262e-01, -1.68090582e-01,\n",
      "          -3.62569690e-02,  1.34783998e-01,  1.37948915e-01,\n",
      "          -9.69914198e-02, -7.96820894e-02, -5.24133444e-04,\n",
      "          -1.75835356e-01, -7.72988573e-02, -4.53917682e-03,\n",
      "          -1.95087820e-01,  7.73176700e-02, -1.89103454e-01,\n",
      "           1.21299937e-01,  9.74225253e-02],\n",
      "         [ 1.87214002e-01, -5.66209257e-02, -3.37815583e-02,\n",
      "          -5.21693379e-02,  3.19856107e-02, -3.64770889e-02,\n",
      "           7.66652972e-02,  2.15639621e-02, -1.91676021e-01,\n",
      "          -1.36809304e-01,  1.05635270e-01,  1.71687320e-01,\n",
      "           9.51214284e-02,  6.29823357e-02, -1.57965422e-01,\n",
      "          -1.09300613e-02, -9.32399407e-02, -8.98049772e-02,\n",
      "          -1.68001011e-01,  2.65100449e-02,  4.46340293e-02,\n",
      "          -1.97332963e-01,  1.95668504e-01,  1.40900508e-01,\n",
      "           4.60942239e-02, -1.42258152e-01, -1.96567625e-01,\n",
      "          -1.40747547e-01, -5.41882068e-02, -4.26412076e-02,\n",
      "           1.41623273e-01, -1.10706680e-01],\n",
      "         [-7.36967325e-02,  1.54718801e-01,  5.99087030e-02,\n",
      "           3.36911380e-02, -5.89930266e-02, -9.49852467e-02,\n",
      "           1.82584152e-01,  6.36915118e-02, -7.63762519e-02,\n",
      "          -1.72393382e-01,  8.09539855e-03,  1.36221036e-01,\n",
      "          -7.44609386e-02,  8.40661377e-02, -7.68490136e-03,\n",
      "           5.40986657e-03, -1.07497379e-01, -1.97810382e-02,\n",
      "           7.83188194e-02, -3.62728536e-03,  1.29662156e-02,\n",
      "          -5.40090501e-02,  3.52182537e-02,  1.15854308e-01,\n",
      "           1.57063082e-01,  1.10937223e-01, -8.94439220e-03,\n",
      "           2.92514265e-02, -1.10237911e-01, -6.64231777e-02,\n",
      "          -7.90006816e-02, -5.29429317e-03],\n",
      "         [ 5.64642251e-03,  1.37260869e-01,  2.38840431e-02,\n",
      "          -7.30995983e-02,  1.49717256e-01,  1.55562714e-01,\n",
      "           1.65037408e-01,  4.39626127e-02, -1.43207669e-01,\n",
      "          -1.87499925e-01, -1.49543598e-01,  3.72157544e-02,\n",
      "           1.43919989e-01,  1.83673695e-01,  5.08894324e-02,\n",
      "           1.13932684e-01,  1.72796741e-01, -6.20822012e-02,\n",
      "          -4.94495481e-02,  1.89339206e-01, -6.77867383e-02,\n",
      "          -4.48652357e-02,  1.81268305e-02, -1.02222376e-01,\n",
      "           1.08653620e-01, -1.90695912e-01, -6.60532266e-02,\n",
      "          -1.78960189e-01,  6.66312128e-02, -5.29387593e-02,\n",
      "          -7.78634995e-02, -8.28102902e-02],\n",
      "         [ 6.06662780e-02,  1.88647375e-01, -1.34432614e-02,\n",
      "           1.23227090e-02, -8.44191238e-02,  1.07496828e-02,\n",
      "           2.12234259e-02, -4.85623330e-02,  2.15225220e-02,\n",
      "           7.48060793e-02, -3.05473804e-05, -1.97050691e-01,\n",
      "           1.93547860e-01,  1.44330606e-01, -1.12975843e-01,\n",
      "           1.54727563e-01,  1.11653522e-01, -1.92078888e-01,\n",
      "           5.63691109e-02,  9.50674266e-02, -1.96811095e-01,\n",
      "           1.91375315e-02,  1.01641670e-01, -1.87887534e-01,\n",
      "          -2.74233520e-03, -6.58881515e-02, -9.85224321e-02,\n",
      "          -6.67448342e-03,  9.10526514e-03, -1.19186908e-01,\n",
      "          -2.53801942e-02, -1.00267597e-01]],\n",
      "\n",
      "        [[-2.47221440e-02, -1.08524010e-01,  1.63183644e-01,\n",
      "          -1.34233519e-01,  8.43110681e-03, -1.12286054e-01,\n",
      "          -5.74768931e-02, -7.58254454e-02,  5.67603111e-03,\n",
      "           6.15242124e-03,  4.35567647e-02,  1.53853849e-01,\n",
      "           4.42895293e-03,  1.24620214e-01, -1.85791552e-02,\n",
      "          -1.28608078e-01,  1.98483512e-01,  4.99988943e-02,\n",
      "           1.89023301e-01,  3.49965245e-02, -7.89623186e-02,\n",
      "           2.30773985e-02,  1.59620211e-01,  1.71308964e-02,\n",
      "          -1.51195437e-01, -1.18458040e-01,  1.59487531e-01,\n",
      "           1.55994162e-01, -1.44764587e-01,  2.46624649e-02,\n",
      "          -1.47295609e-01,  9.22357291e-02],\n",
      "         [ 1.49062321e-01,  4.36003357e-02, -1.01476550e-01,\n",
      "           2.36442238e-02,  1.43316433e-01,  1.45108864e-01,\n",
      "          -1.59010619e-01,  1.65630862e-01,  4.72696275e-02,\n",
      "           1.63997367e-01, -1.06352285e-01, -1.63974166e-01,\n",
      "           1.09011635e-01, -6.29837215e-02,  1.34389549e-02,\n",
      "          -1.77330181e-01, -8.45958591e-02, -5.74045032e-02,\n",
      "          -1.34102583e-01,  1.91911921e-01,  1.28708079e-01,\n",
      "          -2.83323675e-02,  1.52301237e-01,  6.13556951e-02,\n",
      "           9.10331160e-02,  1.88568518e-01, -1.58763722e-01,\n",
      "           2.71569639e-02, -1.43761486e-02,  1.67102516e-02,\n",
      "           1.00583658e-01, -5.33480197e-02],\n",
      "         [-1.02342688e-01,  1.53361261e-03, -1.76830009e-01,\n",
      "           4.23063636e-02,  7.03458637e-02, -1.62637845e-01,\n",
      "          -8.80232379e-02, -8.37216154e-02,  1.46860048e-01,\n",
      "           1.62162319e-01,  7.74523467e-02, -1.40355110e-01,\n",
      "           1.50097445e-01,  1.90786570e-02, -1.00963168e-01,\n",
      "          -7.00885504e-02, -5.81439435e-02,  2.86954045e-02,\n",
      "           1.24108300e-01, -1.18547566e-01, -6.14583492e-02,\n",
      "           1.63850740e-01, -5.07724285e-02, -1.31590515e-01,\n",
      "           1.57682762e-01,  2.35600919e-02,  4.84315008e-02,\n",
      "           1.07308462e-01, -1.84909165e-01, -6.71520978e-02,\n",
      "          -6.20835274e-02,  1.27540931e-01],\n",
      "         [ 1.42620519e-01,  1.54309019e-01,  1.71621904e-01,\n",
      "          -6.22960180e-02, -1.52616560e-01, -1.55532882e-01,\n",
      "           1.60601214e-01,  1.12726167e-01, -1.10333219e-01,\n",
      "           9.43206251e-03, -1.44868314e-01, -9.37841162e-02,\n",
      "           1.60824135e-01, -4.68879342e-02, -6.22718185e-02,\n",
      "          -1.30235851e-01, -1.48883134e-01,  6.56906217e-02,\n",
      "           4.13504541e-02, -2.84598023e-02, -1.87946230e-01,\n",
      "          -1.35489315e-01,  4.38137352e-02, -6.60398304e-02,\n",
      "          -4.04676944e-02,  1.63157538e-01, -1.54317528e-01,\n",
      "           1.79561540e-01, -1.57752261e-01,  1.57325283e-01,\n",
      "          -9.07705501e-02, -1.76606625e-02],\n",
      "         [-1.55636370e-01, -7.80849010e-02, -3.34051698e-02,\n",
      "           1.74952909e-01, -4.97601479e-02,  1.91675201e-01,\n",
      "           9.73948687e-02, -8.71352181e-02,  1.59570888e-01,\n",
      "          -1.01531781e-01, -1.50148273e-02,  1.70606926e-01,\n",
      "           2.89918929e-02, -1.09269768e-02, -1.71221912e-01,\n",
      "          -1.06324621e-01, -1.43488988e-01, -2.41690278e-02,\n",
      "          -1.38913721e-01, -1.12569131e-01, -1.11674219e-02,\n",
      "           1.98538646e-01, -1.59679234e-01, -4.62071002e-02,\n",
      "          -9.27309617e-02, -1.11788012e-01,  5.76965362e-02,\n",
      "          -4.97560799e-02, -1.40474573e-01,  1.37996271e-01,\n",
      "          -1.32237494e-01,  1.47774413e-01],\n",
      "         [ 8.62347931e-02,  1.33880392e-01,  1.14720806e-01,\n",
      "          -5.02110571e-02, -4.97094691e-02,  1.88469693e-01,\n",
      "          -1.42906964e-01, -1.79726437e-01,  1.36840269e-01,\n",
      "           2.42249668e-02,  9.33773071e-02,  1.96359619e-01,\n",
      "          -5.57251424e-02, -8.70677158e-02, -1.08263388e-01,\n",
      "           1.07731059e-01, -1.50586471e-01, -1.52946532e-01,\n",
      "          -2.09707171e-02,  1.55497834e-01,  2.67044902e-02,\n",
      "           7.80210048e-02, -1.76908314e-01, -2.60276794e-02,\n",
      "           1.59253433e-01,  9.45681781e-02, -1.56299219e-01,\n",
      "           1.19509831e-01, -1.17991976e-01, -1.34413719e-01,\n",
      "          -1.94700286e-01,  1.79756418e-01]]]], dtype=float32)>, <tf.Variable 'stock_critic_4/my_model_11/conv2d_22/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_4/my_model_11/batch_normalization_22/gamma:0' shape=(32,) dtype=float32, numpy=\n",
      "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_4/my_model_11/batch_normalization_22/beta:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_4/my_model_11/conv2d_23/kernel:0' shape=(1, 2, 32, 1) dtype=float32, numpy=\n",
      "array([[[[ 0.20560187],\n",
      "         [ 0.28229332],\n",
      "         [-0.0393303 ],\n",
      "         [ 0.16818663],\n",
      "         [-0.0310187 ],\n",
      "         [ 0.27386802],\n",
      "         [-0.1210517 ],\n",
      "         [-0.03685096],\n",
      "         [-0.22617584],\n",
      "         [ 0.2706905 ],\n",
      "         [-0.09130876],\n",
      "         [ 0.22165763],\n",
      "         [ 0.10731402],\n",
      "         [ 0.22711003],\n",
      "         [-0.30129784],\n",
      "         [ 0.15930527],\n",
      "         [ 0.29111207],\n",
      "         [-0.1039044 ],\n",
      "         [-0.08280595],\n",
      "         [ 0.06548354],\n",
      "         [-0.19278118],\n",
      "         [-0.19340336],\n",
      "         [ 0.14676464],\n",
      "         [-0.17691611],\n",
      "         [-0.12934403],\n",
      "         [-0.17483881],\n",
      "         [ 0.19769114],\n",
      "         [ 0.28346205],\n",
      "         [ 0.20460701],\n",
      "         [ 0.1042825 ],\n",
      "         [-0.02444467],\n",
      "         [ 0.12539533]],\n",
      "\n",
      "        [[ 0.12352392],\n",
      "         [-0.15968728],\n",
      "         [ 0.22320521],\n",
      "         [-0.25629532],\n",
      "         [ 0.21396738],\n",
      "         [ 0.03005645],\n",
      "         [-0.19158098],\n",
      "         [-0.0006502 ],\n",
      "         [-0.10195801],\n",
      "         [-0.01442289],\n",
      "         [ 0.22490555],\n",
      "         [ 0.06676275],\n",
      "         [-0.05173126],\n",
      "         [-0.24349424],\n",
      "         [-0.2683066 ],\n",
      "         [ 0.18567374],\n",
      "         [-0.02122813],\n",
      "         [-0.04306331],\n",
      "         [-0.2829931 ],\n",
      "         [ 0.12207371],\n",
      "         [ 0.20710588],\n",
      "         [ 0.23387694],\n",
      "         [-0.11676514],\n",
      "         [-0.09424487],\n",
      "         [-0.24346764],\n",
      "         [-0.28795195],\n",
      "         [-0.2659323 ],\n",
      "         [-0.19062912],\n",
      "         [ 0.2795127 ],\n",
      "         [ 0.0870325 ],\n",
      "         [ 0.00565562],\n",
      "         [-0.09992917]]]], dtype=float32)>, <tf.Variable 'stock_critic_4/my_model_11/conv2d_23/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Variable 'stock_critic_4/my_model_11/batch_normalization_23/gamma:0' shape=(1,) dtype=float32, numpy=array([1.], dtype=float32)>, <tf.Variable 'stock_critic_4/my_model_11/batch_normalization_23/beta:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>, <tf.Variable 'stock_critic_4/dense_24/kernel:0' shape=(3, 10) dtype=float32, numpy=\n",
      "array([[-0.13387743, -0.15178879,  0.28106594, -0.6345275 , -0.48682493,\n",
      "        -0.73708564, -0.16646864,  0.10564827,  0.20903358,  0.43973306],\n",
      "       [-0.792746  ,  0.20020656, -0.20785137, -0.41845688,  0.01178529,\n",
      "        -0.7285828 , -0.01912894,  0.50205445,  0.7284948 ,  0.23509543],\n",
      "       [-0.03099956,  0.18329825, -0.22358209, -0.01002901, -0.23099446,\n",
      "         0.2863487 , -0.12680006,  0.3304553 , -0.14382555, -0.08768391]],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_4/dense_24/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'stock_critic_4/dense_25/kernel:0' shape=(10, 10) dtype=float32, numpy=\n",
      "array([[ 0.21563953, -0.04910442,  0.15994366,  0.09355628, -0.66137654,\n",
      "        -0.58185923, -0.3645254 ,  0.21523064,  0.24745908,  0.56295294],\n",
      "       [-0.5378759 ,  0.21271907, -0.03156934,  0.5834325 ,  0.40656132,\n",
      "         0.1475333 ,  0.00786033,  0.3084694 ,  0.2621145 , -0.13017064],\n",
      "       [ 0.2499539 , -0.37377262, -0.15622881, -0.11300541, -0.2110168 ,\n",
      "        -0.4946614 , -0.31086564,  0.3278714 ,  0.09667097,  0.08673897],\n",
      "       [-0.02881246,  0.57167834,  0.6313278 ,  0.52897745, -0.4283009 ,\n",
      "         0.19912626,  0.11008944,  0.49869174,  0.34123614,  0.25940827],\n",
      "       [-0.3935246 ,  0.04642471, -0.36371255,  0.06108092,  0.39605293,\n",
      "         0.25344402,  0.46773535, -0.21880415,  0.10913058,  0.42309883],\n",
      "       [ 0.12547152,  0.12078465, -0.44671553, -0.23727459,  0.41288605,\n",
      "        -0.45324916, -0.52811   , -0.43116847, -0.12453139,  0.21062721],\n",
      "       [-0.14905013, -0.5261327 ,  0.4317079 ,  0.16915381,  0.34888682,\n",
      "        -0.3143109 , -0.01724367,  0.05198846, -0.0748191 , -0.18329798],\n",
      "       [ 0.16665432, -0.11835975,  0.01817069, -0.2292073 ,  0.21689601,\n",
      "         0.18367682, -0.6368538 , -0.6651842 , -0.29939923, -0.54622227],\n",
      "       [-0.07558581, -0.02048523,  0.19406232,  0.15819658,  0.04791236,\n",
      "        -0.66070133,  0.27270994,  0.29661041, -0.13427791, -0.24232587],\n",
      "       [-0.442544  ,  0.6138667 ,  0.3696783 , -0.21302217,  0.43180034,\n",
      "        -0.69825083,  0.06463668,  0.18681988,  0.47546744,  0.16465126]],\n",
      "      dtype=float32)>, <tf.Variable 'stock_critic_4/dense_25/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'stock_critic_4/dense_26/kernel:0' shape=(10, 1) dtype=float32, numpy=\n",
      "array([[-0.5974547 ],\n",
      "       [ 0.49121025],\n",
      "       [ 0.49747905],\n",
      "       [-0.3363121 ],\n",
      "       [-0.32857803],\n",
      "       [-0.06161327],\n",
      "       [ 0.20932187],\n",
      "       [-0.29039672],\n",
      "       [-0.07826025],\n",
      "       [-0.59928954]], dtype=float32)>, <tf.Variable 'stock_critic_4/dense_26/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['stock_critic_4/my_model_11/conv2d_22/kernel:0', 'stock_critic_4/my_model_11/conv2d_22/bias:0', 'stock_critic_4/my_model_11/batch_normalization_22/gamma:0', 'stock_critic_4/my_model_11/batch_normalization_22/beta:0', 'stock_critic_4/my_model_11/conv2d_23/kernel:0', 'stock_critic_4/my_model_11/conv2d_23/bias:0', 'stock_critic_4/my_model_11/batch_normalization_23/gamma:0', 'stock_critic_4/my_model_11/batch_normalization_23/beta:0', 'stock_critic_4/dense_24/kernel:0', 'stock_critic_4/dense_24/bias:0', 'stock_critic_4/dense_25/kernel:0', 'stock_critic_4/dense_25/bias:0', 'stock_critic_4/dense_26/kernel:0', 'stock_critic_4/dense_26/bias:0'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-58d0c5c70b23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDDPG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-ab758814ed50>\u001b[0m in \u001b[0;36mDDPG\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mQval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;31m#update Q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdateQ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mQval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\P3A-Deep-Reinforcement-Learning\\Stock-Trading-Environment\\networks\\DQNac.py\u001b[0m in \u001b[0;36mupdateQ\u001b[1;34m(self, actual_values, selected_action_values)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    424\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m--> 426\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\reinforcement-learning-master\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[1;32m-> 1039\u001b[1;33m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0m\u001b[0;32m   1040\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     logging.warning(\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['stock_critic_4/my_model_11/conv2d_22/kernel:0', 'stock_critic_4/my_model_11/conv2d_22/bias:0', 'stock_critic_4/my_model_11/batch_normalization_22/gamma:0', 'stock_critic_4/my_model_11/batch_normalization_22/beta:0', 'stock_critic_4/my_model_11/conv2d_23/kernel:0', 'stock_critic_4/my_model_11/conv2d_23/bias:0', 'stock_critic_4/my_model_11/batch_normalization_23/gamma:0', 'stock_critic_4/my_model_11/batch_normalization_23/beta:0', 'stock_critic_4/dense_24/kernel:0', 'stock_critic_4/dense_24/bias:0', 'stock_critic_4/dense_25/kernel:0', 'stock_critic_4/dense_25/bias:0', 'stock_critic_4/dense_26/kernel:0', 'stock_critic_4/dense_26/bias:0']."
     ]
    }
   ],
   "source": [
    "DDPG()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (reinforcement-learning-master)",
   "language": "python",
   "name": "pycharm-7a748578"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
